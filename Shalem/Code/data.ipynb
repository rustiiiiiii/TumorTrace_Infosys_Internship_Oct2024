{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9905837,"sourceType":"datasetVersion","datasetId":6085803},{"sourceId":10029567,"sourceType":"datasetVersion","datasetId":6176918},{"sourceId":179424,"sourceType":"modelInstanceVersion","modelInstanceId":152868,"modelId":175314}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc","metadata":{"_uuid":"819f2470-c646-435b-aa8d-b0eceb07e8e8","_cell_guid":"a889abbf-2e6a-4fc1-a0f4-0af07e552979","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define transformations for training data with augmentation\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),                # Resize to 224x224 pixels\n    transforms.RandomHorizontalFlip(p=0.5),       # Randomly flip images horizontally\n    transforms.RandomRotation(degrees=15),        # Randomly rotate within Â±15 degrees\n    transforms.ColorJitter(brightness=0.2,        # Adjust brightness, contrast, saturation, and hue\n                           contrast=0.2,\n                           saturation=0.2,\n                           hue=0.1),\n    transforms.ToTensor(),                        # Convert image to PyTorch tensor\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\n# Define transformations for validation and test data (no augmentation)\ntest_val_transform = transforms.Compose([\n    transforms.Resize((224, 224)),                # Resize to 224x224 pixels\n    transforms.ToTensor(),                        # Convert image to PyTorch tensor\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])","metadata":{"_uuid":"046e0d29-4695-47ca-aefe-3e8b09c9719d","_cell_guid":"325cc38c-dfd3-4483-b492-723e3f4e797b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets with respective transformations\ntrain_dataset = datasets.ImageFolder(\n    root='/kaggle/input/tumortrace/clasification-roi/train',  # Update the path to your dataset\n    transform=train_transform\n)\nval_dataset = datasets.ImageFolder(\n    root='/kaggle/input/tumortrace/clasification-roi/val',  # Update the path to your dataset\n    transform=test_val_transform\n)\ntest_dataset = datasets.ImageFolder(\n    root='/kaggle/input/tumortrace/clasification-roi/test',  # Update the path to your dataset\n    transform=test_val_transform\n)","metadata":{"_uuid":"b89ee58b-aa6e-43c4-84db-642cb3470a2f","_cell_guid":"8938c74f-543e-4155-ab6d-69f6fbbd4ebc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataLoaders for each dataset\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n\n# Display class names in the dataset\nprint(f\"Available class names: {train_dataset.classes}\")","metadata":{"_uuid":"73153323-1e8d-4799-9ca0-e044c6cdbc8a","_cell_guid":"08286f6c-48eb-4e49-806e-21c8053b2332","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to count images in each class\ndef count_classes(dataset):\n    class_counts = {class_name: 0 for class_name in dataset.classes}\n\n    for _, labels in DataLoader(dataset, batch_size=1):\n        for label in labels:\n            class_name = dataset.classes[label.item()]\n            class_counts[class_name] += 1\n\n    return class_counts\n\n# Count images in each dataset\ntrain_counts = count_classes(train_dataset)\nval_counts = count_classes(val_dataset)\ntest_counts = count_classes(test_dataset)\n\n# Print the counts\nprint(f\"Train dataset counts: {train_counts}\")\nprint(f\"Validation dataset counts: {val_counts}\")\nprint(f\"Test dataset counts: {test_counts}\")","metadata":{"_uuid":"73e51b43-a14e-476f-80f8-a0a05d412f02","_cell_guid":"7423d4f6-d5d1-4df0-a526-ca6a8424c636","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to convert tensor back to PIL image\ndef tensor_to_pil(tensor):\n    unnormalize = transforms.Normalize(\n        mean=[-0.5 / 0.5, -0.5 / 0.5, -0.5 / 0.5],\n        std=[1 / 0.5, 1 / 0.5, 1 / 0.5]\n    )\n    tensor = unnormalize(tensor).clamp(0, 1)  # Undo normalization and clamp to valid range\n    return transforms.ToPILImage()(tensor)    # Convert tensor to PIL image","metadata":{"_uuid":"92b18aa4-86e7-4560-a075-a752bba08b62","_cell_guid":"cba3ac1b-d3c9-4843-8fe2-149b8e5ba23f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to display augmented images\ndef show_augmented_images(dataset, class_name, num_images=5):\n    if class_name not in dataset.classes:\n        raise ValueError(f\"Class '{class_name}' not found. Available classes: {dataset.classes}\")\n\n    plt.figure(figsize=(15, 5))  # Create figure\n\n    # Find the class index for the requested class\n    class_idx = dataset.class_to_idx[class_name]\n\n    # Load a sample image of the requested class\n    for img, label in DataLoader(dataset, batch_size=1, shuffle=True):\n        if label.item() == class_idx:  # Check if the image matches the desired class\n            sample_img = img[0]  # Extract the image tensor\n            pil_img = tensor_to_pil(sample_img)  # Convert tensor to PIL image\n            break\n\n    # Display multiple augmented versions of the same image\n    for i in range(num_images):\n        augmented_img = train_transform(pil_img)  # Apply random transformations again\n\n        plt.subplot(1, num_images, i + 1)  # Create a subplot for each image\n        plt.imshow(augmented_img.permute(1, 2, 0).numpy())  # Convert tensor to numpy\n        plt.axis('off')  # Hide axes\n\n    plt.suptitle(f'Augmented versions of class: {class_name}', fontsize=16)\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"197da73e-fef2-4812-bf16-d66415d2d031","_cell_guid":"f8516239-a13a-4b0b-9c6f-df843cb4481f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display augmented images for the first class\nshow_augmented_images(train_dataset, class_name=train_dataset.classes[0], num_images=5)\n\n# If there are multiple classes, display augmented images for the second class\nif len(train_dataset.classes) > 1:\n    show_augmented_images(train_dataset, class_name=train_dataset.classes[1], num_images=5)","metadata":{"_uuid":"0ee21ba7-f85d-4353-8f28-ca1036c3e755","_cell_guid":"dca1f1c9-4296-46e3-80b0-9a107e9f187b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraries for image processing and visualization\nimport cv2\nimport matplotlib.pyplot as plt\nfrom skimage.feature import hog\nfrom skimage import exposure\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"3b48173c-dfff-4ffb-bed2-1ede73cf5b91","_cell_guid":"cc7e386a-ef8d-4174-94cb-f12bd25fd01f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and resize an image\ndef load_and_resize_image(image_path, size=(224, 224)):\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n    return cv2.resize(image, size)  # Resize to target dimensions","metadata":{"_uuid":"116c6dc9-9e30-4dc6-b97f-1a1946e7d72e","_cell_guid":"8cd41d94-9751-4ac4-8135-bd3b4c882fc2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot resized image and pixel intensity histogram\ndef plot_image_and_histogram(image):\n    pixel_values = image.flatten()\n    \n    plt.figure(figsize=(12, 6))\n    \n    # Plot the resized image\n    plt.subplot(1, 2, 1)\n    plt.imshow(image, cmap='gray')\n    plt.title('Resized Image')\n    plt.axis('off')\n    \n    # Plot the histogram\n    plt.subplot(1, 2, 2)\n    plt.hist(pixel_values, bins=256, range=(0, 256), color='gray', alpha=0.7)\n    plt.title('Histogram of Pixel Values')\n    plt.xlabel('Pixel Intensity')\n    plt.ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"389baa38-8180-4075-886f-01a949cb2547","_cell_guid":"7108389e-77ec-4f86-a9cd-933743454a9d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute and plot HOG features\ndef plot_hog_features(image):\n    hog_features, hog_image = hog(image, orientations=9, pixels_per_cell=(8, 8),\n                                  cells_per_block=(2, 2), visualize=True, block_norm='L2-Hys')\n    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n    \n    plt.figure(figsize=(12, 6))\n    \n    # Original image\n    plt.subplot(1, 2, 1)\n    plt.imshow(image, cmap='gray')\n    plt.title('Original Image')\n    plt.axis('off')\n    \n    # HOG visualization\n    plt.subplot(1, 2, 2)\n    plt.imshow(hog_image_rescaled, cmap='gray')\n    plt.title('HOG Visualization')\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"255af9d3-fa70-419b-87eb-06c57378a2da","_cell_guid":"baa1287d-60d5-437e-8310-370f8d73f409","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = \"/kaggle/input/tumortrace/clasification-roi/test/Benign/BreaDM-Be-1810/SUB1/p-030.jpg\"  # Update with a valid path\nimage_resized = load_and_resize_image(image_path)\n\n# Visualize\nplot_image_and_histogram(image_resized)\nplot_hog_features(image_resized)","metadata":{"_uuid":"089f15b6-b303-4b27-a7df-7258ecce76d2","_cell_guid":"0069076e-21cb-4beb-b8af-c3d74ff98f8e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convolve(image, kernel):\n    \"\"\"Perform convolution on an image with a kernel.\"\"\"\n    image_x, image_y = image.shape\n    kernel_x, kernel_y = kernel.shape\n\n    height_radius, width_radius = np.array(kernel.shape) // 2\n    output = np.zeros(image.shape)\n    \n    # Stride and padding\n    stride = 1\n    padding = 0\n    output_x = int(((image_x - kernel_x + 2 * padding) // stride) + 1)\n    output_y = int(((image_y - kernel_y + 2 * padding) // stride) + 1)\n    \n    print(f\"Output Dimensions: {output_x}x{output_y}\")","metadata":{"_uuid":"62896fe9-37f9-4fc1-a60d-90587635936b","_cell_guid":"64698b7c-fe7a-46c4-960c-b91c42f94611","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_lbp(image):\n    \"\"\"Compute the Local Binary Pattern of an image.\"\"\"\n    height, width = image.shape\n    lbp_image = np.zeros((height - 2, width - 2), dtype=np.uint8)  # Initialize LBP image\n\n    # Iterate over each pixel (excluding the border)\n    for i in range(1, height - 1):\n        for j in range(1, width - 1):\n            center_pixel = image[i, j]\n            binary_string = ''  # Initialize binary string for LBP pattern\n            \n            # Check the 8 neighbors in clockwise order\n            binary_string += '1' if image[i-1, j-1] >= center_pixel else '0'  # Top-left\n            binary_string += '1' if image[i-1, j] >= center_pixel else '0'    # Top-center\n            binary_string += '1' if image[i-1, j+1] >= center_pixel else '0'  # Top-right\n            binary_string += '1' if image[i, j+1] >= center_pixel else '0'    # Middle-right\n            binary_string += '1' if image[i+1, j+1] >= center_pixel else '0'  # Bottom-right\n            binary_string += '1' if image[i+1, j] >= center_pixel else '0'    # Bottom-center\n            binary_string += '1' if image[i+1, j-1] >= center_pixel else '0'  # Bottom-left\n            binary_string += '1' if image[i, j-1] >= center_pixel else '0'    # Middle-left\n\n            # Convert binary string to decimal and assign to the LBP image\n            lbp_value = int(binary_string, 2)\n            lbp_image[i-1, j-1] = lbp_value\n\n    return lbp_image  # Return the computed LBP image","metadata":{"_uuid":"03e0a58e-f09f-4d1b-99dd-0c5657fbbaa3","_cell_guid":"89f7a2f1-3801-47c9-9e58-58d3d7a0e232","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_lbp(image_path):\n    \"\"\"Load an image, compute LBP, and display both.\"\"\"\n    image = load_and_resize_image(image_path)        # Load the image\n    lbp_image = compute_lbp(image)         # Compute LBP\n\n    # Plot the original and LBP images\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.title('Original Image (224x224)')\n    plt.imshow(image, cmap='gray')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.title('Local Binary Pattern (LBP)')\n    plt.imshow(lbp_image, cmap='gray')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndisplay_lbp(image_path)","metadata":{"_uuid":"a5ae473e-3121-4462-8390-c4e695c2b1fe","_cell_guid":"2fb895bb-1d28-47ce-ac24-a40a4817afb6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_mvm_lbp(image):\n    \"\"\"Compute Mean-Variance-Median Local Binary Pattern.\"\"\"\n    height, width = image.shape\n    lbp_mean = np.zeros((height - 2, width - 2), dtype=np.uint8)\n    lbp_variance = np.zeros((height - 2, width - 2), dtype=np.uint8)\n    lbp_median = np.zeros((height - 2, width - 2), dtype=np.uint8)\n\n    for i in range(1, height - 1):\n        for j in range(1, width - 1):\n            window = image[i - 1:i + 2, j - 1:j + 2].flatten()\n            neighbors = np.delete(window, 4)\n\n            mean, variance, median = np.mean(neighbors), np.var(neighbors), np.median(neighbors)\n            lbp_mean[i - 1, j - 1] = int(''.join(['1' if p >= mean else '0' for p in neighbors]), 2)\n            lbp_variance[i - 1, j - 1] = int(''.join(['1' if p >= variance else '0' for p in neighbors]), 2)\n            lbp_median[i - 1, j - 1] = int(''.join(['1' if p >= median else '0' for p in neighbors]), 2)\n\n    return lbp_mean, lbp_variance, lbp_median","metadata":{"_uuid":"adb9846f-53d6-464e-b137-872e5ebf5e23","_cell_guid":"4f8ba214-88d0-4f76-8e3b-0424f29166d4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and preprocess the image\nimage_path = '/kaggle/input/tumortrace/clasification-roi/train/Malignant/BreaDM-Ma-2127/SUB7/p-049.jpg'\n\nimage = load_and_resize_image(image_path)\n\n# Compute and Display LBP\nlbp_image = compute_lbp(image)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.title('Original Image (224x224)')\nplt.imshow(image, cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(lbp_image, cmap='gray')\nplt.title('Local Binary Pattern')\nplt.axis('off')\nplt.show()\n\n# Compute and Display MVM-LBP\nlbp_mean, lbp_variance, lbp_median = compute_mvm_lbp(image)\n\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 4, 1)\nplt.title('Original Image (224x224)')\nplt.imshow(image, cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 4, 2)\nplt.title('Mean-LBP')\nplt.imshow(lbp_mean, cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 4, 3)\nplt.title('Variance-LBP')\nplt.imshow(lbp_variance, cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 4, 4)\nplt.title('Median-LBP')\nplt.imshow(lbp_median, cmap='gray')\nplt.axis('off')\n\nplt.show()","metadata":{"_uuid":"bd75e1da-b3a6-4a11-8d38-72c35524b4e7","_cell_guid":"64497b28-6c25-4f03-a6d2-eb47c38ed082","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom skimage import io, color, transform\nimport matplotlib.pyplot as plt\n\ndef resize_and_normalize(image, size=(224, 224)):\n    \"\"\"\n    Resizes the image to the specified size and normalizes pixel values to 0-255.\n    \"\"\"\n    # Resize the image\n    image_resized = transform.resize(image, size, anti_aliasing=True)\n    \n    # Normalize to range [0, 255] and convert to uint8\n    image_normalized = (image_resized * 255).astype(np.uint8)\n    \n    return image_normalized\n\ndef compute_glcm(image, distance=1, angle=0, levels=256):\n    \"\"\"\n    Computes a Gray-Level Co-occurrence Matrix (GLCM) for a given grayscale image.\n    \n    Parameters:\n    - image: The input grayscale image as a 2D NumPy array.\n    - distance: Pixel distance for GLCM calculation (1 for adjacent pixels).\n    - angle: Angle in degrees (0 for horizontal, 90 for vertical, 45 for diagonal).\n    - levels: Number of gray levels (typically 256 for 8-bit images).\n    \n    Returns:\n    - glcm: A 2D NumPy array representing the GLCM.\n    \"\"\"\n    glcm = np.zeros((levels, levels), dtype=np.int32)\n    rows, cols = image.shape\n    \n    # Define direction based on angle\n    if angle == 0:  # Horizontal\n        row_offset, col_offset = 0, distance\n    elif angle == 90:  # Vertical\n        row_offset, col_offset = distance, 0\n    elif angle == 45:  # Diagonal (bottom-left to top-right)\n        row_offset, col_offset = -distance, distance\n    elif angle == 135:  # Diagonal (top-left to bottom-right)\n        row_offset, col_offset = distance, distance\n    else:\n        raise ValueError(\"Angle must be 0, 45, 90, or 135 degrees.\")\n    \n    # Calculate GLCM\n    for i in range(rows):\n        for j in range(cols):\n            # Determine neighboring pixel based on offsets\n            row_neighbor = i + row_offset\n            col_neighbor = j + col_offset\n            \n            # Check if the neighboring pixel is within image boundaries\n            if 0 <= row_neighbor < rows and 0 <= col_neighbor < cols:\n                current_pixel = image[i, j]\n                neighbor_pixel = image[row_neighbor, col_neighbor]\n                \n                # Accumulate GLCM counts\n                glcm[current_pixel, neighbor_pixel] += 1\n                \n    return glcm\n\n# Load the image\nimage = io.imread('/kaggle/input/tumortrace/clasification-roi/train/Malignant/BreaDM-Ma-2127/SUB7/p-049.jpg')  # Replace 'path_to_image' with the actual path\n\n# Convert to grayscale if it's an RGB image\nif image.ndim == 3:\n    image = color.rgb2gray(image)\n\n# Resize and normalize the image\nimage_processed = resize_and_normalize(image)\n\n# Compute the horizontal GLCM with 1 pixel offset and 0 degrees (horizontal)\nglcm = compute_glcm(image_processed, distance=1, angle=0)\n\n# Print the GLCM matrix to inspect its raw values\nprint(\"Horizontal GLCM (0 degrees):\")\nprint(glcm)","metadata":{"_uuid":"81198ab3-79e2-40a3-b64a-59042b1dd1f3","_cell_guid":"641cadb6-4a2b-405e-a21c-5a9eed625eb4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"VGG MODEL","metadata":{"_uuid":"751479b2-aa11-4b8c-a813-58748c0e4b89","_cell_guid":"6676e14b-fe23-49e3-8755-1bdc5d60848d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Import necessary PyTorch libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom tqdm import tqdm\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc as compute_auc, confusion_matrix , roc_auc_score\nimport numpy as np\nimport os","metadata":{"_uuid":"7e7babdc-5cd5-4444-a4d6-a433a5cb4ec0","_cell_guid":"ab7b31cf-71c3-4dc8-82c1-26a8038d35ec","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the pre-trained VGG-16 model\nvgg16 = models.vgg16(pretrained=True)\n\n# Set the model to evaluation mode\nvgg16.eval()\n\n# Print the VGG-16 architecture\nprint(vgg16)","metadata":{"_uuid":"cdbc2e75-cfd5-4441-abba-aebe684e8d22","_cell_guid":"a603aa07-6d55-463a-b2b7-e583c78d062c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a custom VGG-16 model for a specific number of output classes\nclass CustomVGG16(nn.Module):\n    def __init__(self, num_classes=2):\n        super(CustomVGG16, self).__init__()\n\n        # Load the pre-trained VGG-16 model\n        vgg16 = models.vgg16(pretrained=True)\n        \n        # Extract the features and avgpool layers from the pre-trained model\n        self.features = vgg16.features\n        self.avgpool = vgg16.avgpool\n        \n        # Define a new classifier with custom layers\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),  # First fully connected layer\n            nn.ReLU(inplace=True),         # ReLU activation\n            nn.Dropout(p=0.5),             # Dropout layer\n            nn.Linear(4096, 4096),         # Second fully connected layer\n            nn.ReLU(inplace=True),         # ReLU activation\n            nn.Dropout(p=0.5),             # Dropout layer\n            nn.Linear(4096, num_classes)   # Output layer for classification\n        )\n\n    def forward(self, x):\n        # Forward pass through the feature extraction layers\n        x = self.features(x)\n        \n        # Forward pass through the average pooling layer\n        x = self.avgpool(x)\n        \n        # Flatten the output from avgpool to match the input size of the classifier\n        x = torch.flatten(x, 1)\n        \n        # Forward pass through the custom classifier\n        x = self.classifier(x)\n        \n        return x\n\n# ResNet18 Model\nclass ResNet18(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ResNet18, self).__init__()\n        model_resnet18 = models.resnet18(pretrained=True)\n        self.conv1 = model_resnet18.conv1\n        self.bn1 = model_resnet18.bn1\n        self.relu = model_resnet18.relu\n        self.maxpool = model_resnet18.maxpool\n        self.layer1 = model_resnet18.layer1\n        self.layer2 = model_resnet18.layer2\n        self.layer3 = model_resnet18.layer3\n        self.layer4 = model_resnet18.layer4\n        self.avgpool = model_resnet18.avgpool\n        self.features = model_resnet18.fc.in_features\n        self.fc = nn.Linear(self.features, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# ResNet50 Model\nclass ResNet50(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ResNet50, self).__init__()\n        model_resnet50 = models.resnet50(pretrained=True)\n        self.conv1 = model_resnet50.conv1\n        self.bn1 = model_resnet50.bn1\n        self.relu = model_resnet50.relu\n        self.maxpool = model_resnet50.maxpool\n        self.layer1 = model_resnet50.layer1\n        self.layer2 = model_resnet50.layer2\n        self.layer3 = model_resnet50.layer3\n        self.layer4 = model_resnet50.layer4\n        self.avgpool = model_resnet50.avgpool\n        self.features = model_resnet50.fc.in_features\n        self.fc = nn.Linear(self.features, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x","metadata":{"_uuid":"9d134c46-3bc8-4be9-bd93-1114b91262f0","_cell_guid":"bfb242fe-80e0-4ecc-8274-8982db159836","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create an instance of the custom VGG-16 model\nmodel = CustomVGG16(num_classes=2)\n\n# Print the architecture of the custom model\nprint(model)","metadata":{"_uuid":"35ab8bb0-cecd-40b8-ae9f-683ef559c55f","_cell_guid":"55ac1d88-0379-41ee-9514-6f8f756503d9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nclass EarlyStopping:\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n    def __call__(self, val_loss, model):\n        score = -val_loss  # Minimizing val_loss, so higher \"score\" is better\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n            \n    def reset(self):\n        \n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decreases.\"\"\"\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"_uuid":"938a6f90-dd7f-4526-bdcc-e84623b87e0b","_cell_guid":"8350b1c1-90b6-42c3-a2e4-1e94d89f9e64","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Device Configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"ca37a8dc-5eb8-4f32-afd2-72f7c5418c44","_cell_guid":"f0420afa-8799-499d-972a-eda9e1c5174a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model and Hyperparameter Initialization\nnum_classes = 2\nlr = 0.01\nmomentum = 0.9\nl2_decay = 0.01\ntotal_epochs = 50\nlog_interval = 10","metadata":{"_uuid":"3cf24a6c-b51b-47a4-9b80-9bbb6cebbc3e","_cell_guid":"ea133514-ad88-48cf-aa32-e93e9a2fc799","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Function\ndef train(epoch, model, num_epochs, loader, criterion, l2_decay):\n    learning_rate = max(lr * (0.1 ** (epoch // 10)), 1e-5)\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=l2_decay)\n    model.train()\n\n    correct = 0\n    for data, label in tqdm(loader, desc=f'Epoch {epoch}/{num_epochs}', unit='batch'):\n        data, label = data.float().to(device), label.long().to(device)\n\n        output = model(data)\n        optimizer.zero_grad()\n        loss = F.nll_loss(F.log_softmax(output, dim=1), label)\n        loss.backward()\n        optimizer.step()\n\n        pred = output.data.max(1)[1]\n        correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n\n    accuracy = 100. * correct / len(loader.dataset)\n    print(f\"Epoch {epoch}: Train Accuracy: {accuracy:.2f}%\")","metadata":{"_uuid":"78005b5c-faf0-40fa-a039-8b81a49eae65","_cell_guid":"e2c22567-124a-4c9a-bc1d-8fa004a23e1b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validation Function\ndef validation(model, val_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    all_predictions, all_targets = [], []\n    possibilities = None\n\n    for data, target in val_loader:\n        data, target = data.to(device), target.to(device)\n        val_output = model(data)\n\n        test_loss += F.nll_loss(F.log_softmax(val_output, dim=1), target, reduction='sum').item()\n        pred = val_output.data.max(1)[1]\n        all_predictions.extend(pred.cpu().numpy())\n        all_targets.extend(target.cpu().numpy())\n\n        possibility = F.softmax(val_output, dim=1).cpu().detach().numpy()\n        possibilities = np.concatenate((possibilities, possibility), axis=0) if possibilities is not None else possibility\n\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    cm = confusion_matrix(all_targets, all_predictions)\n\n    num_classes = val_output.shape[1]\n    label_onehot = np.eye(num_classes)[np.array(all_targets).astype(int)]\n    fpr, tpr, _ = roc_curve(label_onehot.ravel(), possibilities.ravel())\n    auc_score = compute_auc(fpr, tpr)\n\n    test_loss /= len(val_loader.dataset)\n    accuracy = 100. * correct / len(val_loader.dataset)\n    specificity = 1 - fpr[1] if len(fpr) > 1 else 0\n    sensitivity = tpr[1] if len(tpr) > 1 else 0\n\n    print(f\"Validation: Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%, AUC: {auc_score:.4f}\")\n    print(f\"Confusion Matrix:\\n{cm}\")  # Print confusion matrix\n    print(f\"Specificity: {specificity:.4f}, Sensitivity: {sensitivity:.4f}\")\n    return test_loss, accuracy, cm, auc_score","metadata":{"_uuid":"7153f4d4-2d47-45ee-89c9-12deca93713b","_cell_guid":"2425d56e-69d7-4e0a-9faf-effc5abf926a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom model (replace `customVGG16` with your model class)\nmodel = CustomVGG16(num_classes=num_classes)\nmodel = model.to(device)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Early stopping setup (implement EarlyStopping in your environment)\nearly_stop = EarlyStopping(patience=20, verbose=True)\n\n# Project and model names\nproject_name = 'tumor_classification'\nmodel_name = 'vgg16'\n\n# Model Training and Evaluation\nbest_accuracy = 0\nmodel_save_dir = os.path.join('model', project_name, model_name)\nos.makedirs(model_save_dir, exist_ok=True)\n\nfor epoch in range(1, total_epochs + 1):\n    train(epoch, model, total_epochs, train_loader, criterion, l2_decay)\n\n    with torch.no_grad():\n        test_loss, accuracy, cm, auc = validation(model, val_loader)\n\n    # Save the model if it achieves the best AUC\n    model_dict = model.module.state_dict() if isinstance(model, nn.parallel.DistributedDataParallel) else model.state_dict()\n    if auc > best_accuracy:\n        best_accuracy = auc\n        torch.save(model_dict, os.path.join(model_save_dir, f'{model_name}_{epoch}.pth'))\n\n    early_stop(test_loss, model)\n    if early_stop.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"_uuid":"bfb0a27c-3278-451d-a294-b39cd26a6b29","_cell_guid":"6dd12626-8f08-4f43-9e89-f37947446c02","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize ResNet18 Model\nmodel = ResNet18(num_classes=num_classes)\nmodel = model.to(device)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Early stopping setup (implement EarlyStopping in your environment)\nearly_stop = EarlyStopping(patience=20, verbose=True)\n\n# Project and model names\nproject_name = 'tumor_classification'\nmodel_name = 'Resenet18'\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\nbest_accuracy = 0\nmodel_save_dir = os.path.join('model', project_name, model_name)\nos.makedirs(model_save_dir, exist_ok=True)\n\n\n# Training loop\nfor epoch in range(1, total_epochs + 1):\n    train(epoch, model, total_epochs, train_loader, criterion, l2_decay)\n    with torch.no_grad():\n        test_loss, accuracy, cm, auc = validation(model, val_loader)\n    # Save the model if it achieves the best AUC\n    model_dict = model.module.state_dict() if isinstance(model, nn.parallel.DistributedDataParallel) else model.state_dict()\n    if auc > best_accuracy:\n        best_accuracy = auc\n        torch.save(model_dict, os.path.join(model_save_dir, f'{model_name}_{epoch}.pth'))\n    early_stop(test_loss, model)\n    if early_stop.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"_uuid":"d06572ff-591d-4ea3-a812-abb6aac41d63","_cell_guid":"872fdec5-2a9e-4404-aaaa-1040ff42b4f7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize ResNet18 Model\nmodel = ResNet50(num_classes=num_classes)\nmodel = model.to(device)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Early stopping setup (implement EarlyStopping in your environment)\nearly_stop = EarlyStopping(patience=15, verbose=True)\n\n# Project and model names\nproject_name = 'tumor_classification'\nmodel_name = 'Resenet50'\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\nbest_accuracy = 0\nmodel_save_dir = os.path.join('model', project_name, model_name)\nos.makedirs(model_save_dir, exist_ok=True)\n\n\n# Training loop\nfor epoch in range(1, total_epochs + 1):\n    train(epoch, model, total_epochs, train_loader, criterion, l2_decay)\n    with torch.no_grad():\n        test_loss, accuracy, cm, auc = validation(model, val_loader)\n    # Save the model if it achieves the best AUC\n    model_dict = model.module.state_dict() if isinstance(model, nn.parallel.DistributedDataParallel) else model.state_dict()\n    if auc > best_accuracy:\n        best_accuracy = auc\n        torch.save(model_dict, os.path.join(model_save_dir, f'{model_name}_{epoch}.pth'))\n    early_stop(test_loss, model)\n    if early_stop.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"_uuid":"7cdcee78-6490-48ae-b7b5-29e045c5ff15","_cell_guid":"a9a15d56-51b0-40c4-947d-7367e737b65b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ndef test_with_visualization(model, test_dataloader, device='cpu'):\n    model.eval()\n\n    test_loss = 0\n    correct = 0\n    possibilities = None\n    all_predictions = []\n    all_true_labels = []  # Store all true labels\n\n    for data, target in test_dataloader:\n        if torch.cuda.is_available():\n            data, target = data.to(device), target.to(device)\n\n        test_output = model(data)\n        test_loss += F.nll_loss(F.log_softmax(test_output, dim=1), target, reduction='sum').item()\n\n        pred = test_output.data.max(1)[1]\n        all_predictions.append(pred.cpu().numpy())\n        all_true_labels.append(target.cpu().numpy())\n\n        possibility = F.softmax(test_output, dim=1).cpu().data.numpy()\n        if possibilities is None:\n            possibilities = possibility\n        else:\n            possibilities = np.concatenate((possibilities, possibility), axis=0)\n\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    # Flatten all predictions and true labels\n    all_predictions = np.concatenate(all_predictions)\n    all_true_labels = np.concatenate(all_true_labels)\n\n    # Classification metrics -> accuracy, f1 score\n    print(f\"Results for {model.__class__.__name__}:\")\n    print(metrics.classification_report(all_true_labels, all_predictions, target_names=['benign', 'malignant'], digits=4))\n\n    # Confusion matrix\n    cm = metrics.confusion_matrix(all_true_labels, all_predictions)\n    print(f\"Confusion Matrix for {model.__class__.__name__}:\\n\", cm)\n\n    # Plot Confusion Matrix\n    plt.figure(figsize=(6, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(\"Confusion Matrix\")\n    plt.colorbar()\n    tick_marks = np.arange(len(set(all_true_labels)))\n    plt.xticks(tick_marks, tick_marks)\n    plt.yticks(tick_marks, tick_marks)\n    plt.ylabel(\"True Label\")\n    plt.xlabel(\"Predicted Label\")\n    plt.show()\n\n    # ROC curve and AUC\n    num_classes = test_output.shape[1]\n    label_onehot = np.eye(num_classes)[all_true_labels]\n\n    fpr, tpr, _ = roc_curve(label_onehot.ravel(), possibilities.ravel())\n    auc_value = roc_auc_score(label_onehot, possibilities, average=\"macro\")\n\n    # Plot ROC Curve\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_value:.4f})\")\n    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    test_loss /= len(test_dataloader.dataset)\n    print(f'Specificity: {1 - fpr[0]:.4f}, Sensitivity: {tpr[0]:.4f}, AUC: {auc_value:.4f}')\n    print(f'\\n{model.__class__.__name__} set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_dataloader.dataset)} ({100. * correct / len(test_dataloader.dataset):.2f}%)\\n')\n\n    return 100. * correct / len(test_dataloader.dataset), test_loss, auc_value\n","metadata":{"_uuid":"bd3ab064-09ec-4674-9fcf-e2a9826ede3b","_cell_guid":"6984e7d8-ae5c-4df2-b31a-eae3b2e6b3b1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_accuracy, test_loss, test_auc = test_with_visualization(model, test_loader, device='cuda' if torch.cuda.is_available() else 'cpu')\n\nif test_accuracy is not None:\n    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}