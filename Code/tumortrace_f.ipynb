{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"private_outputs":true,"provenance":[],"gpuType":"V28","mount_file_id":"14LWwAD_E4klL0zzK1LyQH6yFhEdZhnX1","authorship_tag":"ABX9TyMINi9kggaIyOLXoXv/LeqD"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9963874,"sourceType":"datasetVersion","datasetId":6129129},{"sourceId":10042858,"sourceType":"datasetVersion","datasetId":6186732},{"sourceId":182829,"sourceType":"modelInstanceVersion","modelInstanceId":155836,"modelId":178292},{"sourceId":182834,"sourceType":"modelInstanceVersion","modelInstanceId":155840,"modelId":178296}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import datasets, transforms","metadata":{"id":"QVYZ__oNHiti","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:31:52.661480Z","iopub.execute_input":"2024-11-29T18:31:52.661848Z","iopub.status.idle":"2024-11-29T18:31:56.784560Z","shell.execute_reply.started":"2024-11-29T18:31:52.661811Z","shell.execute_reply":"2024-11-29T18:31:56.783677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:33:51.815216Z","iopub.execute_input":"2024-11-29T18:33:51.815998Z","iopub.status.idle":"2024-11-29T18:36:19.015950Z","shell.execute_reply.started":"2024-11-29T18:33:51.815962Z","shell.execute_reply":"2024-11-29T18:36:19.014609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:36:23.182243Z","iopub.execute_input":"2024-11-29T18:36:23.182896Z","iopub.status.idle":"2024-11-29T18:36:23.187969Z","shell.execute_reply.started":"2024-11-29T18:36:23.182860Z","shell.execute_reply":"2024-11-29T18:36:23.186801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"TVaXAyVrNXjl","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:36:27.060046Z","iopub.execute_input":"2024-11-29T18:36:27.060378Z","iopub.status.idle":"2024-11-29T18:36:27.764526Z","shell.execute_reply.started":"2024-11-29T18:36:27.060342Z","shell.execute_reply":"2024-11-29T18:36:27.763607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport os","metadata":{"id":"BNTZhG1PN7nD","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:36:30.159934Z","iopub.execute_input":"2024-11-29T18:36:30.160619Z","iopub.status.idle":"2024-11-29T18:36:30.164274Z","shell.execute_reply.started":"2024-11-29T18:36:30.160586Z","shell.execute_reply":"2024-11-29T18:36:30.163440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = '/kaggle/input/tumor-trace-data'","metadata":{"id":"y22zcK2AU7Qg","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:36:54.148714Z","iopub.execute_input":"2024-11-29T18:36:54.149048Z","iopub.status.idle":"2024-11-29T18:36:54.152877Z","shell.execute_reply.started":"2024-11-29T18:36:54.149021Z","shell.execute_reply":"2024-11-29T18:36:54.152032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset","metadata":{"id":"RG4boOLaYPZT","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:36:39.836187Z","iopub.execute_input":"2024-11-29T18:36:39.836610Z","iopub.status.idle":"2024-11-29T18:36:39.841340Z","shell.execute_reply.started":"2024-11-29T18:36:39.836557Z","shell.execute_reply":"2024-11-29T18:36:39.840476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**loading custom dataset**","metadata":{"id":"sgDSf2ka_Hby"}},{"cell_type":"code","source":"class BreastCancerDataset(Dataset):\n    def __init__(self, dataset_path, transform=None):\n        self.dataset_path = dataset_path\n        self.transform = transform\n        self.samples = self._load_samples()\n        print(f\"Number of samples found: {len(self.samples)}\")\n        if len(self.samples) == 0:\n            raise ValueError(f\"No valid samples found in {self.dataset_path}\")\n\n    def _load_samples(self):\n        samples = []\n        for label in ['Benign', 'Malignant']:\n            label_dir = os.path.join(self.dataset_path, label)\n            if not os.path.exists(label_dir):\n                continue\n            for root, _, files in os.walk(label_dir):\n                for file in files:\n                    if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n                        file_path = os.path.join(root, file)\n                        samples.append((file_path, 0 if label == 'Benign' else 1))\n        return samples\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        image_path, label = self.samples[idx]\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"id":"z2Aht7HStq7V","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:36:49.357909Z","iopub.execute_input":"2024-11-29T18:36:49.358232Z","iopub.status.idle":"2024-11-29T18:36:49.365817Z","shell.execute_reply.started":"2024-11-29T18:36:49.358203Z","shell.execute_reply":"2024-11-29T18:36:49.364984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\ntrain_dataset = BreastCancerDataset('/kaggle/input/tumor-trace-data/clasification-roi/train', transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataset = BreastCancerDataset('/kaggle/input/tumor-trace-data/clasification-roi/test', transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\nval_dataset = BreastCancerDataset('/kaggle/input/tumor-trace-data/clasification-roi/val', transform=transform)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\nfor images, labels in train_loader:\n    print(images.size(), labels)  # Print the size of the batch and the labels\n    break  # Just show the first batch","metadata":{"id":"4SGy-9NTwjai","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:36:59.249117Z","iopub.execute_input":"2024-11-29T18:36:59.249973Z","iopub.status.idle":"2024-11-29T18:37:11.083157Z","shell.execute_reply.started":"2024-11-29T18:36:59.249940Z","shell.execute_reply":"2024-11-29T18:37:11.082329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimages, labels = next(iter(train_loader))\nimage = images[3]\nimage = image.numpy().transpose((1, 2, 0))\nplt.imshow(image)\nplt.title(f\"Label: {'Benign' if labels[0] == 0 else 'Malignant'}\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:58:42.522086Z","iopub.execute_input":"2024-11-29T18:58:42.522433Z","iopub.status.idle":"2024-11-29T18:58:42.911309Z","shell.execute_reply.started":"2024-11-29T18:58:42.522388Z","shell.execute_reply":"2024-11-29T18:58:42.910404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n# Define the dataset path (update as per your dataset in Kaggle)\ndataset_path = \"/kaggle/input/tumor-trace-data/clasification-roi\"  # Adjust this to match your Kaggle dataset path\n\n# Define transformations for the training dataset\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomCrop(210),  # Randomly crop images to 210x210\n    transforms.RandomHorizontalFlip(),  # Horizontal flipping\n    transforms.RandomRotation(15),  # Random rotation\n    transforms.ToTensor(),  # Convert images to tensors\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n])\n\n# Define transformations for validation and test datasets\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),  # Convert images to tensors\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n])\n\nvalid_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),  # Convert images to tensors\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n])\n\n# Load datasets\ntrain_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=train_transform)\ntest_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'test'), transform=test_transform)\nvalid_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'val'), transform=valid_transform)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n\n# Count classes in datasets\nbenign_count_train = train_dataset.targets.count(0)\nmalignant_count_train = train_dataset.targets.count(1)\nprint(f'Train Set: Benign = {benign_count_train}, Malignant = {malignant_count_train}')\n\nbenign_count_test = test_dataset.targets.count(0)\nmalignant_count_test = test_dataset.targets.count(1)\nprint(f'Test Set: Benign = {benign_count_test}, Malignant = {malignant_count_test}')\n\nbenign_count_valid = valid_dataset.targets.count(0)\nmalignant_count_valid = valid_dataset.targets.count(1)\nprint(f'Validation Set: Benign = {benign_count_valid}, Malignant = {malignant_count_valid}')\n\n# Function to display sample images\ndef display_sample_images(dataset, num_images=5):\n    plt.figure(figsize=(15, 5))\n    for i in range(num_images):\n        image, label = dataset[i]\n        # Reverse the normalization for display purposes\n        image = image.permute(1, 2, 0).numpy()\n        image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # De-normalize\n        image = (image * 255).astype('uint8')  # Convert to 8-bit integer format for display\n        plt.subplot(1, num_images, i + 1)\n        plt.imshow(image)\n        plt.title(dataset.classes[label])\n        plt.axis('off')\n    plt.show()\n\n# Display sample images from the training dataset\ndisplay_sample_images(train_dataset)\n\n# Plot class distributions\nlabels = ['Benign', 'Malignant']\ntrain_counts = [benign_count_train, malignant_count_train]\ntest_counts = [benign_count_test, malignant_count_test]\nvalid_counts = [benign_count_valid, malignant_count_valid]\n\nx = range(len(labels))\n\nplt.figure(figsize=(10, 5))\nplt.bar(x, train_counts, width=0.2, label='Train', align='center')\nplt.bar([p + 0.2 for p in x], test_counts, width=0.2, label='Test', align='center')\nplt.bar([p + 0.4 for p in x], valid_counts, width=0.2, label='Validation', align='center')\n\nplt.xlabel('Class')\nplt.ylabel('Number of Cases')\nplt.title('Count of Benign and Malignant Cases')\nplt.xticks([p + 0.2 for p in x], labels)\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:51:04.918040Z","iopub.execute_input":"2024-11-29T18:51:04.918825Z","iopub.status.idle":"2024-11-29T18:51:07.603186Z","shell.execute_reply.started":"2024-11-29T18:51:04.918795Z","shell.execute_reply":"2024-11-29T18:51:07.602312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the image path in the Kaggle input directory\nimage_path = '/kaggle/input/tumor-trace-data/clasification-roi/train/Benign/BreaDM-Be-1801/SUB1/p-034.jpg'\n\n# Load the image in grayscale mode\nimage = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n# Check if the image was loaded correctly\nif image is None:\n    print(f\"Error: Could not load image from path: {image_path}\")\nelse:\n    # Calculate the histogram\n    hist = cv2.calcHist([image], [0], None, [256], [0, 256]).flatten()\n\n    # Get the top N most frequent pixel values\n    top_n = 5\n    most_frequent_indices = np.argsort(hist)[-top_n:][::-1]\n\n    # Plot the histogram\n    plt.figure(figsize=(10, 5))\n    plt.bar(range(256), hist, color='blue', width=1.0)\n    plt.title(\"Grayscale Histogram\")\n    plt.xlabel(\"Pixel Value\")\n    plt.ylabel(\"Frequency\")\n    plt.xlim([0, 255])\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.show()\n\n    # Print the most frequent pixel values and their frequencies\n    print(\"Top 5 Most Frequent Pixel Values and Their Frequencies:\")\n    for idx in most_frequent_indices:\n        print(f\"Pixel Value: {idx}, Frequency: {int(hist[idx])}\")\n\n    # Visualize the original image\n    plt.figure(figsize=(6, 6))\n    plt.imshow(image, cmap='gray')\n    plt.title(\"Original Image\")\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:56:13.178663Z","iopub.execute_input":"2024-11-29T18:56:13.179011Z","iopub.status.idle":"2024-11-29T18:56:13.759054Z","shell.execute_reply.started":"2024-11-29T18:56:13.178982Z","shell.execute_reply":"2024-11-29T18:56:13.757948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom skimage.feature import hog\nfrom skimage import color\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the dataset path in Kaggle\ndataset_path = \"/kaggle/input/tumor-trace-data/clasification-roi\"  # Adjust this to match your Kaggle dataset\n\n# Define transformations for resizing images (HOG operates on fixed-size images)\nresize_transform = transforms.Compose([\n    transforms.Resize((128, 128)),  # HOG works well with smaller images\n    transforms.ToTensor()\n])\n\n# Load datasets\ntrain_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'train'), transform=resize_transform)\ntest_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'test'), transform=resize_transform)\nvalid_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'val'), transform=resize_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n\n# Function to extract HOG features from a dataset image\ndef extract_hog_features(image):\n    image_np = image.permute(1, 2, 0).numpy()  # Convert tensor to NumPy array\n    image_gray = color.rgb2gray(image_np)  # Convert RGB image to grayscale\n    hog_features, hog_image = hog(image_gray, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2),\n                                  block_norm='L2-Hys', visualize=True, feature_vector=True)\n    return hog_features, hog_image\n\n# Display HOG features for sample images\ndef display_hog_sample(dataset, num_images=5):\n    plt.figure(figsize=(15, 10))\n    for i in range(num_images):\n        image, label = dataset[i]\n        hog_features, hog_image = extract_hog_features(image)\n        \n        # Plot the original image\n        plt.subplot(2, num_images, i + 1)\n        plt.imshow(image.permute(1, 2, 0).numpy())  # Original image\n        plt.title(dataset.classes[label])\n        plt.axis('off')\n\n        # Plot the HOG visualization\n        plt.subplot(2, num_images, i + num_images + 1)\n        plt.imshow(hog_image, cmap='gray')  # HOG visualization\n        plt.title('HOG Features')\n        plt.axis('off')\n    plt.show()\n\n# Example of HOG features for training images\ndisplay_hog_sample(train_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:57:32.097438Z","iopub.execute_input":"2024-11-29T18:57:32.097759Z","iopub.status.idle":"2024-11-29T18:57:35.210945Z","shell.execute_reply.started":"2024-11-29T18:57:32.097731Z","shell.execute_reply":"2024-11-29T18:57:35.210084Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EDA**","metadata":{"id":"4rM_uUo4PWVR"}},{"cell_type":"code","source":"from skimage import color, exposure\nfrom skimage.feature import hog, corner_harris, corner_peaks\nfrom skimage import color, exposure\nfrom skimage.feature import hog, corner_harris, corner_peaks ","metadata":{"id":"7utcMzwPEZLb","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:59:12.326006Z","iopub.execute_input":"2024-11-29T18:59:12.326813Z","iopub.status.idle":"2024-11-29T18:59:12.330912Z","shell.execute_reply.started":"2024-11-29T18:59:12.326779Z","shell.execute_reply":"2024-11-29T18:59:12.330076Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"** histogram of pixel values**","metadata":{"id":"PObzmzgwPW9v"}},{"cell_type":"markdown","source":"Calculate GLCM properties","metadata":{"id":"6aCy36jWjL2x"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.feature import graycomatrix, graycoprops\nfrom skimage.color import rgb2gray\nimport torch\n\n# Get a batch of images and labels from train_loader\nimages, labels = next(iter(train_loader))\n\n# Select the first image from the batch\nimage = images[0]\n\n# Convert the image from a PyTorch tensor to a NumPy array and grayscale it\nimage = image.permute(1, 2, 0).cpu().numpy()  # Convert to HxWxC format and ensure it's on CPU\ngray_image = rgb2gray(image)  # Convert RGB image to grayscale\n\n# Plot the original image and GLCM properties\nplt.figure(figsize=(12, 6))\n\n# Plot the original image\nplt.subplot(1, 3, 1)\nplt.imshow(image)\nplt.title(f\"Label: {'Benign' if labels[0].item() == 0 else 'Malignant'}\")\nplt.axis('off')\n\n# Calculate GLCM and GLCM properties\ndistances = [1]  # Pixel pair distance\nangles = [0]  # Angle (0 degrees)\nglcm = graycomatrix((gray_image * 255).astype(np.uint8), distances=distances, angles=angles, symmetric=True, normed=True)\n\n# Extract GLCM properties\ncontrast = graycoprops(glcm, 'contrast')[0, 0]\ncorrelation = graycoprops(glcm, 'correlation')[0, 0]\nenergy = graycoprops(glcm, 'energy')[0, 0]\nhomogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n\n# Display GLCM properties\nplt.subplot(1, 3, 3)\nplt.text(0.1, 0.8, f'Contrast: {contrast:.4f}', fontsize=12)\nplt.text(0.1, 0.6, f'Correlation: {correlation:.4f}', fontsize=12)\nplt.text(0.1, 0.4, f'Energy: {energy:.4f}', fontsize=12)\nplt.text(0.1, 0.2, f'Homogeneity: {homogeneity:.4f}', fontsize=12)\nplt.axis('off')\nplt.title(\"GLCM Properties\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"w5aBVnizOD52","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:48:10.592933Z","iopub.execute_input":"2024-11-29T19:48:10.593505Z","iopub.status.idle":"2024-11-29T19:48:11.078531Z","shell.execute_reply.started":"2024-11-29T19:48:10.593472Z","shell.execute_reply":"2024-11-29T19:48:11.077729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.feature import graycomatrix, graycoprops\nfrom skimage.color import rgb2gray\nimport torch\n\n# Get a batch of images and labels\nimages, labels = next(iter(train_loader))\n\n# Define parameters for GLCM calculation\ndistances = [1]  # Pixel pair distance\nangles = [0]  # Angle (0 degrees)\n\n# Number of images to process\nnum_images = 5\n\n# Plot the images and their GLCM properties\nplt.figure(figsize=(10, num_images * 3))\n\nfor idx in range(num_images):\n    # Select the image and label\n    image = images[idx]\n    label = labels[idx].item()\n\n    # Convert the image from a PyTorch tensor to a NumPy array and grayscale it\n    image = image.permute(1, 2, 0).cpu().numpy()  # Convert to HxWxC format\n    gray_image = rgb2gray(image)  # Convert RGB image to grayscale\n\n    # Calculate GLCM and GLCM properties\n    glcm = graycomatrix(\n        (gray_image * 255).astype(np.uint8), \n        distances=distances, \n        angles=angles, \n        symmetric=True, \n        normed=True\n    )\n    contrast = graycoprops(glcm, 'contrast')[0, 0]\n    correlation = graycoprops(glcm, 'correlation')[0, 0]\n    energy = graycoprops(glcm, 'energy')[0, 0]\n    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n\n    # Plot the image\n    plt.subplot(num_images, 2, idx * 2 + 1)\n    plt.imshow(image)\n    plt.title(f\"Image {idx+1}: {'Benign' if label == 0 else 'Malignant'}\")\n    plt.axis('off')\n\n    # Plot the GLCM properties\n    plt.subplot(num_images, 2, idx * 2 + 2)\n    plt.text(0.1, 0.8, f'Contrast: {contrast:.4f}', fontsize=10)\n    plt.text(0.1, 0.6, f'Correlation: {correlation:.4f}', fontsize=10)\n    plt.text(0.1, 0.4, f'Energy: {energy:.4f}', fontsize=10)\n    plt.text(0.1, 0.2, f'Homogeneity: {homogeneity:.4f}', fontsize=10)\n    plt.axis('off')\n    plt.title(\"GLCM Properties\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"ZMNrVqWeH4dR","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:49:10.192623Z","iopub.execute_input":"2024-11-29T19:49:10.192985Z","iopub.status.idle":"2024-11-29T19:49:11.337879Z","shell.execute_reply.started":"2024-11-29T19:49:10.192956Z","shell.execute_reply":"2024-11-29T19:49:11.337117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" HOG image for visualization","metadata":{"id":"CxcyxcdbZCTF"}},{"cell_type":"markdown","source":"coner dectection\n\n\n","metadata":{"id":"kwceBYLKPxnY"}},{"cell_type":"code","source":"from skimage.feature import corner_harris, corner_peaks  # Importing corner detection functions\nfrom skimage.color import rgb2gray\nimport matplotlib.pyplot as plt\n\n# Get a batch of images and labels from the DataLoader\nimages, labels = next(iter(train_loader))\n\n# Select a specific image and its label\nimage = images[5]  # Selecting the 6th image (index 5)\nimage = image.permute(1, 2, 0).cpu().numpy()  # Convert tensor to HxWxC format and ensure it's on CPU\ngray_image = rgb2gray(image)  # Convert RGB image to grayscale\n\n# Perform Harris corner detection\ncorners = corner_harris(gray_image)\n\n# Perform corner peak detection\ncoords = corner_peaks(corners, min_distance=5)\n\n# Plot the original image and the detected corners\nplt.figure(figsize=(12, 5))\n\n# Original grayscale image\nplt.subplot(1, 2, 1)\nplt.imshow(gray_image, cmap='gray')\nplt.title('Original Image')\nplt.axis('off')\n\n# Image with detected corners\nplt.subplot(1, 2, 2)\nplt.imshow(gray_image, cmap='gray')\nplt.scatter(coords[:, 1], coords[:, 0], s=15, c='red', marker='o')\nplt.title('Corner Detection')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Display the number of corners detected\nprint(f'Number of corners detected: {len(coords)}')\n","metadata":{"id":"DPhfiGJh64JD","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:49:43.652375Z","iopub.execute_input":"2024-11-29T19:49:43.653202Z","iopub.status.idle":"2024-11-29T19:49:44.119900Z","shell.execute_reply.started":"2024-11-29T19:49:43.653167Z","shell.execute_reply":"2024-11-29T19:49:44.119102Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sobel operator","metadata":{"id":"q2qxhh2kY5lA"}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef local_binary_pattern(image, radius=1, neighbors=8):\n    height, width = image.shape\n    lbp_image = np.zeros((height, width), dtype=np.uint8)\n    angle_step = 2 * np.pi / neighbors\n\n    for i in range(radius, height - radius):\n        for j in range(radius, width - radius):\n            center_pixel = image[i, j]\n            lbp_code = 0\n\n            for n in range(neighbors):\n                y = int(i + radius * np.sin(n * angle_step))\n                x = int(j + radius * np.cos(n * angle_step))\n                if image[y, x] >= center_pixel:\n                    lbp_code |= 1 << n\n\n            lbp_image[i, j] = lbp_code\n\n    return lbp_image\n\n# Define the input image path in the Kaggle input directory\ninput_image_path = '/kaggle/input/tumor-trace-data/clasification-roi/train/Benign/BreaDM-Be-1801/SUB6/p-035.jpg'\n\n# Read the image in grayscale\nimage = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n\n# Check if the image was loaded successfully\nif image is None:\n    raise FileNotFoundError(f\"Image not found at path: {input_image_path}. Please check the file path.\")\n\n# Calculate the LBP\nlbp_image = local_binary_pattern(image)\n\n# Display the original and LBP images\nplt.figure(figsize=(8, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(image, cmap='gray')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(lbp_image, cmap='gray')\nplt.title('Local Binary Pattern (LBP)')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"zOgOMxgkpzde","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:08:34.916592Z","iopub.execute_input":"2024-11-29T19:08:34.917433Z","iopub.status.idle":"2024-11-29T19:08:35.366608Z","shell.execute_reply.started":"2024-11-29T19:08:34.917399Z","shell.execute_reply":"2024-11-29T19:08:35.365713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef harris_corner_detection(image, block_size=2, ksize=3, k=0.04):\n    gray_image = np.float32(image)\n    harris_response = cv2.cornerHarris(gray_image, block_size, ksize, k)\n    harris_response = cv2.dilate(harris_response, None)\n    corners_image = image.copy()\n    corners_image[harris_response > 0.01 * harris_response.max()] = 255\n    return corners_image\n\ndef canny_edge_detection(image, lower_threshold=100, upper_threshold=200):\n    edges = cv2.Canny(image, lower_threshold, upper_threshold)\n    return edges\n\n# Define the input image path in the Kaggle input directory\ninput_image_path = '/kaggle/input/tumor-trace-data/clasification-roi/train/Benign/BreaDM-Be-1801/SUB6/p-035.jpg'\n\n# Read the image in grayscale\nimage = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n\n# Check if the image was loaded successfully\nif image is None:\n    raise FileNotFoundError(f\"Image not found at path: {input_image_path}. Please check the file path.\")\n\n# Apply Harris corner detection\ncorners_image = harris_corner_detection(image)\n\n# Apply Canny edge detection\nedges_image = canny_edge_detection(image)\n\n# Convert images for display in Jupyter Notebook\ncorners_image_rgb = cv2.cvtColor(corners_image, cv2.COLOR_GRAY2RGB)\nedges_image_rgb = cv2.cvtColor(edges_image, cv2.COLOR_GRAY2RGB)\n\n# Display the images using Matplotlib\nplt.figure(figsize=(8, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_GRAY2RGB))\nplt.title('Original Image')\n\nplt.subplot(1, 3, 2)\nplt.imshow(corners_image_rgb)\nplt.title('Harris Corners Detected')\n\nplt.subplot(1, 3, 3)\nplt.imshow(edges_image_rgb)\nplt.title('Canny Edges Detected')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"lxaudq1WL1vk","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:07:56.060374Z","iopub.execute_input":"2024-11-29T19:07:56.060732Z","iopub.status.idle":"2024-11-29T19:07:56.502838Z","shell.execute_reply.started":"2024-11-29T19:07:56.060699Z","shell.execute_reply":"2024-11-29T19:07:56.502017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Define Sobel kernels\nsobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\nsobel_y = np.array([[-1, -2, -1], [0,  0,  0], [1,  2,  1]])\n\n# Function to apply convolution between image and kernel\ndef apply_convolution(image, kernel):\n    height, width = image.shape\n    kernel_height, kernel_width = kernel.shape\n    output_image = np.zeros((height, width))\n    for i in range(1, height-1):\n        for j in range(1, width-1):\n            region = image[i-1:i+2, j-1:j+2]  # 3x3 region around the pixel\n            output_image[i, j] = np.sum(region * kernel)\n    return output_image\n\n# Function to compute gradient magnitude from Sobel outputs\ndef compute_gradient_magnitude(sobel_x_output, sobel_y_output):\n    return np.sqrt(sobel_x_output**2 + sobel_y_output**2)\n\n# Function to normalize the image for display\ndef normalize_image(image):\n    image_min = np.min(image)\n    image_max = np.max(image)\n    normalized_image = (image - image_min) / (image_max - image_min) * 255\n    return normalized_image.astype(np.uint8)\n\n# Define the input image path in Kaggle\ninput_image_path = '/kaggle/input/tumor-trace-data/clasification-roi/train/Malignant/BreaDM-Ma-1802/SUB1/p-035.jpg'\n\n# Load the image\nimage = Image.open(input_image_path).convert('L')  # Convert to grayscale\n\n# Convert the image to a NumPy array\nimage_array = np.array(image)\n\n# Apply Sobel operators\nsobel_x_output = apply_convolution(image_array, sobel_x)\nsobel_y_output = apply_convolution(image_array, sobel_y)\n\n# Calculate gradient magnitude (Sobel magnitude)\nsobel_magnitude = compute_gradient_magnitude(sobel_x_output, sobel_y_output)\n\n# Normalize the output image for better visualization\nnormalized_output = normalize_image(sobel_magnitude)\n\n# Display the results\nplt.figure(figsize=(8, 4))\nplt.subplot(1, 3, 1)\nplt.imshow(image_array, cmap='gray')\nplt.title('Original Image')\n\nplt.subplot(1, 3, 2)\nplt.imshow(sobel_magnitude, cmap='gray')\nplt.title('Sobel Gradient Magnitude')\n\nplt.subplot(1, 3, 3)\nplt.imshow(normalized_output, cmap='gray')\nplt.title('Normalized Output')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:10:43.107584Z","iopub.execute_input":"2024-11-29T19:10:43.107981Z","iopub.status.idle":"2024-11-29T19:10:43.609176Z","shell.execute_reply.started":"2024-11-29T19:10:43.107950Z","shell.execute_reply":"2024-11-29T19:10:43.608349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef lbp_scratch(image):\n    \"\"\"\n    Computes the Local Binary Pattern (LBP) for an input image from scratch.\n    \"\"\"\n    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  # Ensure the image is in grayscale\n    imgLBP = np.zeros_like(gray_image)  # Initialize the LBP image\n\n    neighboor = 3  # Define the size of the neighborhood (3x3 window)\n\n    # Iterate over each pixel in the grayscale image, excluding the border pixels\n    for ih in range(0, gray_image.shape[0] - neighboor):\n        for iw in range(0, gray_image.shape[1] - neighboor):\n            # Extract a 3x3 region centered on the current pixel\n            img = gray_image[ih:ih + neighboor, iw:iw + neighboor]\n            center = img[1, 1]  # The center pixel value of the 3x3 region\n\n            # Binary operation - Compare each neighbor's value with the center pixel\n            img01 = (img >= center) * 1.0  # Threshold the 3x3 region\n\n            # Flatten the thresholded image into a vector, excluding the center pixel\n            img01_vector = img01.flatten()\n            img01_vector = np.delete(img01_vector, 4)  # Remove the center pixel\n\n            # Convert the binary pattern to a decimal number\n            where_img01_vector = np.where(img01_vector)[0]  # Indices where the value is 1\n            if len(where_img01_vector) >= 1:\n                num = np.sum(2 ** where_img01_vector)  # Convert binary to decimal\n            else:\n                num = 0  # If all surrounding pixels are smaller, assign 0\n\n            # Assign the LBP value to the central pixel of the output image\n            imgLBP[ih + 1, iw + 1] = num\n\n    return imgLBP\n\n# Get a batch of images and labels from the train_loader\nimages, labels = next(iter(train_loader))\n\n# Select an image from the batch (ensure it's properly sized)\nimage = images[4].numpy().transpose((1, 2, 0))\n\n# Compute LBP using the custom function\nlbp_image = lbp_scratch(image)\n\n# Display the original and LBP images\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(image)\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(lbp_image, cmap='gray')\nplt.title('Local Binary Pattern (LBP)')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"1sw35Ck_Utse","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:14:51.512436Z","iopub.execute_input":"2024-11-29T19:14:51.513071Z","iopub.status.idle":"2024-11-29T19:14:52.249529Z","shell.execute_reply.started":"2024-11-29T19:14:51.513034Z","shell.execute_reply":"2024-11-29T19:14:52.248723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef binary_pattern(image_path, method='mean'):\n    try:\n        # Load the image in grayscale\n        img_array = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n        if img_array is None:\n            raise FileNotFoundError(f\"Image at {image_path} not found.\")\n        \n        img_array = cv2.resize(img_array, (224, 224))\n\n        # Get the dimensions of the image\n        rows, cols = img_array.shape\n\n        # Initialize the output image\n        bp_image = np.zeros((rows, cols), dtype=np.uint8)\n\n        # Loop over the image, skipping the border pixels\n        for i in range(1, rows - 1):\n            for j in range(1, cols - 1):\n                # Extract the 3x3 neighborhood\n                surr = img_array[i-1:i+2, j-1:j+2]\n\n                # Calculate the reference value based on the selected method\n                if method == 'mean':\n                    ref_val = np.mean(surr)\n                elif method == 'median':\n                    ref_val = np.median(surr)\n                elif method == 'variance':\n                    ref_val = np.var(surr)\n                else:\n                    raise ValueError(\"Invalid method! Use 'mean', 'median', or 'variance'.\")\n\n                # Initialize the binary pattern\n                binary_pattern = 0\n\n                # Create the binary pattern by comparing neighbors to the reference value\n                for ind, (dy, dx) in enumerate([(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]):\n                    if surr[1 + dy, 1 + dx] >= ref_val:\n                        binary_pattern += 2 ** ind\n\n                # Set the binary pattern value for the current pixel\n                bp_image[i, j] = binary_pattern\n\n        # Normalize the output image for display\n        bp_image_normalized = cv2.normalize(bp_image, None, 0, 255, cv2.NORM_MINMAX)\n\n        # Display the original and binary pattern images\n        plt.figure(figsize=(8, 4))\n        plt.subplot(1, 2, 1)\n        plt.imshow(img_array, cmap='gray')\n        plt.title('Original Image')\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(bp_image_normalized, cmap='gray')\n        plt.title(f'{method.capitalize()} Binary Pattern')\n\n        plt.show()\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nimage_path = '/kaggle/input/tumor-trace-data/clasification-roi/train/Benign/BreaDM-Be-1812/SUB1/p-025.jpg'\nbinary_pattern(image_path, method='mean')     # For Mean Binary Pattern\nbinary_pattern(image_path, method='median')   # For Median Binary Pattern\nbinary_pattern(image_path, method='variance') # For Variance Binary Pattern\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:13:35.191025Z","iopub.execute_input":"2024-11-29T19:13:35.191399Z","iopub.status.idle":"2024-11-29T19:13:38.988159Z","shell.execute_reply.started":"2024-11-29T19:13:35.191370Z","shell.execute_reply":"2024-11-29T19:13:38.987387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef mvm_lbp(gray_image):\n    \"\"\"\n    Computes the Local Binary Pattern (LBP) using the Mean-Variance-Median (MVM) threshold.\n    \"\"\"\n    rows, cols = gray_image.shape\n    imgLBP = np.zeros((rows, cols), dtype=np.uint8)\n\n    for ih in range(1, rows - 1):\n        for iw in range(1, cols - 1):\n            # Get the 3x3 neighborhood\n            img = gray_image[ih-1:ih+2, iw-1:iw+2]\n\n            # Calculate Mean, Variance, and Median\n            mean_value = np.mean(img)\n            variance_value = np.var(img)\n            median_value = np.median(img)\n\n            # Calculate MVM threshold\n            mvm_threshold = (mean_value + np.sqrt(variance_value) + median_value) / 3\n\n            # Create a binary pattern by thresholding with MVM threshold\n            img01 = (img >= mvm_threshold).astype(int)\n\n            # Flatten the binary pattern and exclude the center pixel\n            img01_vector = np.delete(img01.flatten(), 4)\n\n            # Convert the binary pattern to a decimal number\n            num = np.dot(img01_vector, 2 ** np.arange(8))\n\n            # Assign the LBP value to the output image\n            imgLBP[ih, iw] = num\n\n    return imgLBP\n\n# Define the input image path in Kaggle\nimage_path = '/kaggle/input/tumor-trace-data/clasification-roi/train/Benign/BreaDM-Be-1812/SUB1/p-025.jpg'\n\n# Read the grayscale image\ngray_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\nif gray_image is None:\n    raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n\n# Compute the MVM LBP\nlbp_image = mvm_lbp(gray_image)\n\n# Display the original and LBP images\nplt.figure(figsize=(6, 3))\nplt.subplot(1, 2, 1)\nplt.imshow(gray_image, cmap='gray')\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(lbp_image, cmap='gray')\nplt.title('MVM LBP Image')\n\nplt.show()\n","metadata":{"id":"FaDwWFKGEyjW","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:56:32.379185Z","iopub.execute_input":"2024-11-29T19:56:32.379506Z","iopub.status.idle":"2024-11-29T19:56:32.689548Z","shell.execute_reply.started":"2024-11-29T19:56:32.379480Z","shell.execute_reply":"2024-11-29T19:56:32.688818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Mean-based LBP calculation\ndef mean_based_lbp(image_array):\n    rows, cols = image_array.shape\n    lbp_image = np.zeros((rows, cols), dtype=np.uint8)\n\n    for i in range(1, rows - 1):\n        for j in range(1, cols - 1):\n            neighborhood = image_array[i-1:i+2, j-1:j+2]\n            mean_value = np.mean(neighborhood)\n            surrounding_pixels = np.delete(neighborhood.flatten(), 4)\n            binary_pattern = (surrounding_pixels >= mean_value).astype(int)\n            lbp_image[i, j] = (binary_pattern * (2 ** np.arange(8))).sum()\n\n    return lbp_image\n\n# Median-based LBP calculation\ndef median_based_lbp(image_array):\n    rows, cols = image_array.shape\n    lbp_image = np.zeros((rows, cols), dtype=np.uint8)\n\n    for i in range(1, rows - 1):\n        for j in range(1, cols - 1):\n            neighborhood = image_array[i-1:i+2, j-1:j+2]\n            median_value = np.median(neighborhood)\n            surrounding_pixels = np.delete(neighborhood.flatten(), 4)\n            binary_pattern = (surrounding_pixels >= median_value).astype(int)\n            lbp_image[i, j] = (binary_pattern * (2 ** np.arange(8))).sum()\n\n    return lbp_image\n\n# Variance-based LBP calculation\ndef variance_based_lbp(image_array):\n    rows, cols = image_array.shape\n    lbp_image = np.zeros((rows, cols), dtype=np.uint8)\n\n    for i in range(1, rows - 1):\n        for j in range(1, cols - 1):\n            neighborhood = image_array[i-1:i+2, j-1:j+2]\n            variance_value = np.var(neighborhood)\n            surrounding_pixels = np.delete(neighborhood.flatten(), 4)\n            binary_pattern = (surrounding_pixels >= variance_value).astype(int)\n            lbp_image[i, j] = (binary_pattern * (2 ** np.arange(8))).sum()\n\n    return lbp_image\n\n# Display function to visualize the results\ndef display_results(original_image, lbp_image, title):\n    lbp_image_normalized = (lbp_image / lbp_image.max()) * 255\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_image, cmap='gray')\n    plt.title('Original Image')\n    plt.subplot(1, 2, 2)\n    plt.imshow(lbp_image_normalized, cmap='gray')\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\n# Load and process image for each LBP method\nimage_path = '/kaggle/input/tumor-trace-data/clasification-roi/train/Benign/BreaDM-Be-1812/SUB1/p-026.jpg'  # Kaggle dataset path\noriginal_image = Image.open(image_path).convert('L').resize((224, 224))\noriginal_image_array = np.array(original_image)\n\n# Mean-based LBP\nmean_lbp_image = mean_based_lbp(original_image_array)\ndisplay_results(original_image_array, mean_lbp_image, 'Mean-based LBP Image')\n\n# Median-based LBP\nmedian_lbp_image = median_based_lbp(original_image_array)\ndisplay_results(original_image_array, median_lbp_image, 'Median-based LBP Image')\n\n# Variance-based LBP\nvariance_lbp_image = variance_based_lbp(original_image_array)\ndisplay_results(original_image_array, variance_lbp_image, 'Variance-based LBP Image')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T19:50:09.794352Z","iopub.execute_input":"2024-11-29T19:50:09.794718Z","iopub.status.idle":"2024-11-29T19:50:15.786396Z","shell.execute_reply.started":"2024-11-29T19:50:09.794682Z","shell.execute_reply":"2024-11-29T19:50:15.785535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\n# Define transformations for the train, test, and validation datasets\ntrain_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),    # Random horizontal flip\n    transforms.RandomRotation(degrees=15),     # Random rotation\n    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)), # Random resized crop\n    transforms.ToTensor(),                     # Convert to PyTorch tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize(224),                    # Resize the shorter edge to 224 pixels\n    transforms.CenterCrop(224),                # Center crop to 224x224\n    transforms.ToTensor(),                     # Convert to PyTorch tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize(224),                    # Resize the shorter edge to 224 pixels\n    transforms.CenterCrop(224),                # Center crop to 224x224\n    transforms.ToTensor(),                     # Convert to PyTorch tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize\n])\n","metadata":{"id":"CnOoca1Jb8Eu","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:00:13.441217Z","iopub.execute_input":"2024-11-29T20:00:13.441937Z","iopub.status.idle":"2024-11-29T20:00:13.448169Z","shell.execute_reply.started":"2024-11-29T20:00:13.441903Z","shell.execute_reply":"2024-11-29T20:00:13.447347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:00:19.157512Z","iopub.execute_input":"2024-11-29T20:00:19.158227Z","iopub.status.idle":"2024-11-29T20:00:19.162087Z","shell.execute_reply.started":"2024-11-29T20:00:19.158193Z","shell.execute_reply":"2024-11-29T20:00:19.161075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = BreastCancerDataset('/kaggle/input/tumor-trace-data/clasification-roi/train', transform=train_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\ntest_dataset = BreastCancerDataset('/kaggle/input/tumor-trace-data/clasification-roi/test', transform=test_transforms)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nval_dataset = BreastCancerDataset('/kaggle/input/tumor-trace-data/clasification-roi/val', transform=val_transforms)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Example: Iterate through the train_loader to see some batches\nfor images, labels in train_loader:\n    print(f\"Batch size: {images.size()}\")  # Prints the size of the batch\n    print(f\"Labels: {labels}\")  # Prints the labels for the batch\n    break  # Display only the first batch","metadata":{"id":"nWl9lNvB8b2M","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:03:36.659820Z","iopub.execute_input":"2024-11-29T20:03:36.660159Z","iopub.status.idle":"2024-11-29T20:03:40.552031Z","shell.execute_reply.started":"2024-11-29T20:03:36.660130Z","shell.execute_reply":"2024-11-29T20:03:40.551108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\n\n# For ResNet18\nresnet18 = models.resnet18(pretrained=True)\nresnet18.eval()  # Set the model to evaluation mode\nprint(\"ResNet18 Model:\")\nprint(resnet18)","metadata":{"id":"DJCYhnGk7sUD","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:03:48.590392Z","iopub.execute_input":"2024-11-29T20:03:48.591030Z","iopub.status.idle":"2024-11-29T20:03:48.831127Z","shell.execute_reply.started":"2024-11-29T20:03:48.590997Z","shell.execute_reply":"2024-11-29T20:03:48.830258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\nimport torchvision.models as models\n\nclass Resnet18(nn.Module):\n    def __init__(self, num_classes=2):\n        super(Resnet18, self).__init__()\n        model_resnet18 = models.resnet18(pretrained=True)\n        self.conv1 = model_resnet18.conv1  # convolutional function\n        self.bn1 = model_resnet18.bn1  # batch normalization\n        self.relu = model_resnet18.relu  # relu is your activation function.\n        self.maxpool = model_resnet18.maxpool  # maxpool is basically taking the biggest value per\n        \n        # sub_matrix\n        self.layer1 = model_resnet18.layer1\n        self.layer2 = model_resnet18.layer2\n        self.layer3 = model_resnet18.layer3\n        self.layer4 = model_resnet18.layer4  # these layers are used for deepening the layers in the architecture which will increase\n        \n        self.avgpool = model_resnet18.avgpool\n        self.features = model_resnet18.fc.in_features\n        self.fc = nn.Linear(self.features, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:04:12.482840Z","iopub.execute_input":"2024-11-29T20:04:12.483740Z","iopub.status.idle":"2024-11-29T20:04:12.490841Z","shell.execute_reply.started":"2024-11-29T20:04:12.483694Z","shell.execute_reply":"2024-11-29T20:04:12.489886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Resnet18()\n\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:04:16.066594Z","iopub.execute_input":"2024-11-29T20:04:16.066949Z","iopub.status.idle":"2024-11-29T20:04:16.328514Z","shell.execute_reply.started":"2024-11-29T20:04:16.066921Z","shell.execute_reply":"2024-11-29T20:04:16.327708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Resnet50(nn.Module):\n    def __init__(self, num_classes=2):\n        super(Resnet50, self).__init__()\n        model_resnet50 = models.resnet50(pretrained=True)\n        self.conv1 = model_resnet50.conv1\n        self.bn1 = model_resnet50.bn1\n        self.relu = model_resnet50.relu\n        self.maxpool = model_resnet50.maxpool\n        self.layer1 = model_resnet50.layer1\n        self.layer2 = model_resnet50.layer2\n        self.layer3 = model_resnet50.layer3\n        self.layer4 = model_resnet50.layer4\n        self.avgpool = model_resnet50.avgpool\n        self.features = model_resnet50.fc.in_features\n        self.fc = nn.Linear(self.features, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:04:23.970108Z","iopub.execute_input":"2024-11-29T20:04:23.970721Z","iopub.status.idle":"2024-11-29T20:04:23.977264Z","shell.execute_reply.started":"2024-11-29T20:04:23.970686Z","shell.execute_reply":"2024-11-29T20:04:23.976412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Resnet50()\n\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:04:29.526188Z","iopub.execute_input":"2024-11-29T20:04:29.526993Z","iopub.status.idle":"2024-11-29T20:04:30.514268Z","shell.execute_reply.started":"2024-11-29T20:04:29.526959Z","shell.execute_reply":"2024-11-29T20:04:30.513377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model training**","metadata":{"id":"-Zk_27AcIdoG"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass customVGG16(nn.Module):\n    def __init__(self, num_classes=2):\n        super(customVGG16, self).__init__()\n\n        # Load the pre-trained VGG16 model\n        vgg16 = models.vgg16(pretrained=True)\n\n        # Extract features and avgpool layers\n        self.features = vgg16.features\n        self.avgpool = vgg16.avgpool\n\n        # Define a new classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),  # Linear layer with input size 512 7 7 and output size 4096\n            nn.ReLU(),                    # ReLU activation function\n            nn.Dropout(),                 # Dropout\n            nn.Linear(4096, 4096),        # Another linear layer with input size 4096 and output size 4096\n            nn.ReLU(),                    # ReLU activation\n            nn.Dropout(),                 # Dropout layer\n            nn.Linear(4096, num_classes)  # Final Linear layer with output size equal to number of classes\n        )\n\n    # Forward Method (Make sure this is outside the __init__ method)\n    def forward(self, x):\n        # Pass input through the features layer\n        x = self.features(x)\n        # Pass through the AVGpool layer\n        x = self.avgpool(x)\n        # Reshape output to a 2D tensor\n        x = torch.flatten(x, 1)\n        # Pass through the classifier\n        x = self.classifier(x)\n        return x\n","metadata":{"id":"3bQF7wf_I4v7","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:04:41.371134Z","iopub.execute_input":"2024-11-29T20:04:41.371832Z","iopub.status.idle":"2024-11-29T20:04:41.378457Z","shell.execute_reply.started":"2024-11-29T20:04:41.371797Z","shell.execute_reply":"2024-11-29T20:04:41.377557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = customVGG16()\n\nprint(model)","metadata":{"id":"HWPsGmjsWySn","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:04:50.431296Z","iopub.execute_input":"2024-11-29T20:04:50.432297Z","iopub.status.idle":"2024-11-29T20:04:55.708008Z","shell.execute_reply.started":"2024-11-29T20:04:50.432251Z","shell.execute_reply":"2024-11-29T20:04:55.707150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nclass EarlyStopping:\n    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        self.patience = patience\n        self.verbose = verbose\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        # Initialize best_score if not set, and check for improvement\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                self.trace_func(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decreases.\"\"\"\n        if self.verbose:\n            self.trace_func(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...\")\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss\n","metadata":{"id":"TLFdUfblbp16","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:08:44.527485Z","iopub.execute_input":"2024-11-29T20:08:44.528081Z","iopub.status.idle":"2024-11-29T20:08:44.535492Z","shell.execute_reply.started":"2024-11-29T20:08:44.528046Z","shell.execute_reply":"2024-11-29T20:08:44.534700Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"training the VGG16 model","metadata":{"id":"qhtpWIpPNxea"}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom tqdm import tqdm","metadata":{"id":"7kxMCmb3rRyJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:08:50.606864Z","iopub.execute_input":"2024-11-29T20:08:50.607719Z","iopub.status.idle":"2024-11-29T20:08:50.611533Z","shell.execute_reply.started":"2024-11-29T20:08:50.607681Z","shell.execute_reply":"2024-11-29T20:08:50.610694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)","metadata":{"id":"B1ft5LBixIGF","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:08:54.939119Z","iopub.execute_input":"2024-11-29T20:08:54.939453Z","iopub.status.idle":"2024-11-29T20:08:55.376529Z","shell.execute_reply.started":"2024-11-29T20:08:54.939422Z","shell.execute_reply":"2024-11-29T20:08:55.375607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n#from tqdm import tqdm #already imported in previous cell\nimport torch.optim as optim\n#from tqdm import tqdm  #already imported in previous cell\n\n# Get the CPU device\n\n#device = torch.device(\"cpu\")\n\n#print(f\"Using device: {device}\")\n\n# ... rest of your code using 'device' for tensors ...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:08:59.470628Z","iopub.execute_input":"2024-11-29T20:08:59.471452Z","iopub.status.idle":"2024-11-29T20:08:59.475329Z","shell.execute_reply.started":"2024-11-29T20:08:59.471417Z","shell.execute_reply":"2024-11-29T20:08:59.474441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:09:03.912321Z","iopub.execute_input":"2024-11-29T20:09:03.912629Z","iopub.status.idle":"2024-11-29T20:09:03.918342Z","shell.execute_reply.started":"2024-11-29T20:09:03.912603Z","shell.execute_reply":"2024-11-29T20:09:03.917459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train function","metadata":{"id":"QVo-Hbt27MOP"}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\nepoch = 0\n\ntotal_epochs = 50\n\nloader = train_loader  # Ensure this is a DataLoader instance for training data\n\ncriterion = nn.CrossEntropyLoss()\n\nl2_decay = 0.01\n\nlr = 0.01  # Learning rate\n\n\n\ndef train(epoch, model, num_epochs, loader, criterion, l2_decay):\n\n    learing_rate = max(lr*(0.1**(epoch//10)),1e-5)\n    optimizer = torch.optim.SGD(model.parameters(), lr= learing_rate, momentum=0.9, weight_decay=l2_decay)\n\n    model.train()\n\n    correct = 0\n    for data, label in tqdm(loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch'):\n        data = data.float().cuda()\n\n        label = label.long().cuda()\n\n        output = model(data)\n        optimizer.zero_grad()\n        loss = F.nll_loss(F.log_softmax(output, dim=1), label)\n        loss.backward()\n        optimizer.step()\n\n        pred = output.data.max(1)[1]\n        correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n\n    print(f'train accuracy: {100. * correct / len(loader.dataset)}%')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:09:08.716592Z","iopub.execute_input":"2024-11-29T20:09:08.717397Z","iopub.status.idle":"2024-11-29T20:09:08.724119Z","shell.execute_reply.started":"2024-11-29T20:09:08.717365Z","shell.execute_reply":"2024-11-29T20:09:08.723221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc as compute_auc  # Rename the imported 'auc' function\nimport sklearn.metrics as metrics\n\ndef validation(model, val_loader):\n    model.eval()  # Set model to evaluation mode\n    test_loss = 0\n    correct = 0\n    all_predictions = []  # Store all predictions\n    all_targets = []  # Store all targets\n    possibilities = None  # Store probabilities for AUC\n\n    for data, target in val_loader:\n        if torch.cuda.is_available():\n            data, target = data.cuda(), target.cuda()\n\n        val_output = model(data)\n\n        # Calculate test loss\n        test_loss += F.nll_loss(F.log_softmax(val_output, dim=1), target, reduction='sum').item()\n\n        # Get predictions and accumulate them\n        pred = val_output.data.max(1)[1]\n        all_predictions.extend(pred.cpu().numpy())  # Collect all predictions\n        all_targets.extend(target.cpu().numpy())  # Collect all target labels\n\n        # Calculate probabilities for AUC\n        possibility = F.softmax(val_output, dim=1).cpu().detach().numpy()\n        if possibilities is None:\n            possibilities = possibility\n        else:\n            possibilities = np.concatenate((possibilities, possibility), axis=0)\n\n        # Calculate the number of correct predictions\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    # Compute confusion matrix\n    cm = metrics.confusion_matrix(all_targets, all_predictions)\n\n    # One-hot encode the labels for AUC computation\n    num_classes = val_output.shape[1]\n    label_onehot = np.eye(num_classes)[np.array(all_targets).astype(int)]\n\n    # Compute ROC curve and AUC\n    fpr, tpr, thresholds = roc_curve(label_onehot.ravel(), possibilities.ravel())\n    auc_score = compute_auc(fpr, tpr)  # Use 'compute_auc' to avoid conflicts\n\n    # Average test loss per sample\n    test_loss /= len(val_loader.dataset)\n\n    # Calculate specificity and sensitivity\n    specificity = 1 - fpr[1] if len(fpr) > 1 else 0\n    sensitivity = tpr[1] if len(tpr) > 1 else 0\n\n    print('Specificity: {:.4f}, Sensitivity: {:.4f}, AUC: {:.4f}'.format(specificity, sensitivity, auc_score))\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(test_loss, 100. * correct / len(val_loader.dataset)))\n\n    return test_loss, 100. * correct / len(val_loader.dataset), cm, auc_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:09:13.516796Z","iopub.execute_input":"2024-11-29T20:09:13.517714Z","iopub.status.idle":"2024-11-29T20:09:13.644318Z","shell.execute_reply.started":"2024-11-29T20:09:13.517676Z","shell.execute_reply":"2024-11-29T20:09:13.643468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_epochs = 50\n\nlr = 0.01\n\nmomentum = 0.9\n\nno_cuda = False\n\nnum_classes=2\n\nlog_interval = 10\n\nl2_decay = 0.01\n\nmodel = customVGG16(num_classes=num_classes)\n\nmodel = model.to(device)\n\n\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"id":"mcIlXpzChJ_7","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T20:09:19.394762Z","iopub.execute_input":"2024-11-29T20:09:19.395094Z","iopub.status.idle":"2024-11-29T20:09:22.205378Z","shell.execute_reply.started":"2024-11-29T20:09:19.395064Z","shell.execute_reply":"2024-11-29T20:09:22.204688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink\n\n# Model training\nmodel.to(device)  \n\nbest_accuracy = 0\nearly_stop = EarlyStopping(patience=20, verbose=True)\n\nproject_name = 'tumor_classification'\nmodel_name = 'vgg16'\n\n# Set Kaggle working directory\nos.chdir(r'/kaggle/working')\n\nfor epoch in range(1, total_epochs + 1):\n    # Training step\n    train(epoch, model, total_epochs, train_loader, criterion, l2_decay)\n\n    # Validation step\n    with torch.no_grad():\n        test_loss, accuracy, cm, auc = validation(model, val_loader)\n\n    # Handle model state for single/multiple GPUs\n    model_state_dict = model.module.state_dict() if isinstance(model, nn.parallel.DistributedDataParallel) else model.state_dict()\n\n    # Save directory for models\n    model_save_dir = os.path.join('model', project_name, model_name)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n\n    # Early stopping check\n    early_stop(test_loss, model)\n\n    # Save the best model based on AUC\n    if auc > best_accuracy:\n        best_accuracy = auc\n        model_save_path = os.path.join(model_save_dir, f'{model_name}_epoch_{epoch}.pth')\n        torch.save(model_state_dict, model_save_path, _use_new_zipfile_serialization=False)\n        print(f\"Model saved at: {os.path.abspath(model_save_path)}\")\n\n        # Generate download link for Kaggle\n        print(\"Generating download link for the saved model...\")\n        display(FileLink(model_save_path))\n\n    # Stop training if early stopping is triggered\n    if early_stop.early_stop:\n        print(\"Early stopping\")\n        break\n\n# Save the model at the final epoch if early stopping is triggered\nfinal_model_save_path = os.path.join(model_save_dir, f'{model_name}_final_epoch.pth')\ntorch.save(model_state_dict, final_model_save_path, _use_new_zipfile_serialization=False)\nprint(f\"Final model saved at: {os.path.abspath(final_model_save_path)}\")\n\n# Generate download link for the final model\nprint(\"Generating download link for the final model...\")\ndisplay(FileLink(final_model_save_path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T21:08:36.748293Z","iopub.execute_input":"2024-11-29T21:08:36.749018Z","iopub.status.idle":"2024-11-29T22:33:00.895847Z","shell.execute_reply.started":"2024-11-29T21:08:36.748982Z","shell.execute_reply":"2024-11-29T22:33:00.894439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import metrics\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nimport torch\n\nimport numpy as np\n\nimport torch.nn.functional as F\n\n\n\ndef test(model, test_loader):\n    name = 'test'\n    len_test_loader = len(test_loader.dataset)\n    model.eval()\n\n    test_loss = 0\n    correct = 0\n    possibilities = None\n    all_predictions = []\n    true_labels = []  # Collect true labels here\n    class_names = ['benign', 'malignant']\n\n    for data, target in test_loader:\n        if torch.cuda.is_available():\n            data, target = data.cuda(), target.cuda()\n\n        test_output = model(data)\n        test_loss += F.nll_loss(F.log_softmax(test_output, dim=1), target, reduction='sum').item()\n\n        pred = test_output.data.max(1)[1]\n        all_predictions.extend(pred.cpu().numpy())  # Collect predictions\n        true_labels.extend(target.cpu().numpy())  # Collect true labels\n\n        possibility = F.softmax(test_output, dim=1).cpu().data.numpy()\n        if possibilities is None:\n            possibilities = possibility\n        else:\n            possibilities = np.concatenate((possibilities, possibility), axis=0)\n\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    # Ensure all_predictions and true_labels are flattened\n    all_predictions = np.array(all_predictions)\n    true_labels = np.array(true_labels)\n\n    # Classification metrics -> accuracy, F1 score\n    print(metrics.classification_report(true_labels, all_predictions, target_names=class_names, digits=4))\n\n    # Confusion matrix\n    cm = metrics.confusion_matrix(true_labels, all_predictions)\n    print(\"\\nConfusion Matrix:\\n\", cm)\n\n    # ROC Curve and AUC\n    num_classes = len(class_names)\n    label_onehot = np.eye(num_classes)[true_labels.astype(int)]\n    fpr, tpr, thresholds = roc_curve(label_onehot.ravel(), possibilities.ravel())\n    auc_value = roc_auc_score(label_onehot, possibilities, average=\"macro\")\n\n    # Specificity, Sensitivity, AUC\n    test_loss /= len_test_loader\n    print('Specificity: {:.4f}, Sensitivity: {:.4f}, AUC: {:.4f}'.format(1 - fpr[0], tpr[0], auc_value))\n    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        name, test_loss, correct, len_test_loader, 100. * correct / len_test_loader))\n\n    #return 100. * correct / len_test_loader, test_loss, auc_value\n    return 100. * correct / len_test_loader, test_loss, auc_value, fpr, tpr\n\n","metadata":{"id":"8-7MT-numHzb","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T22:33:13.068457Z","iopub.execute_input":"2024-11-29T22:33:13.068811Z","iopub.status.idle":"2024-11-29T22:33:13.078822Z","shell.execute_reply.started":"2024-11-29T22:33:13.068778Z","shell.execute_reply":"2024-11-29T22:33:13.077838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T22:33:18.891778Z","iopub.execute_input":"2024-11-29T22:33:18.892366Z","iopub.status.idle":"2024-11-29T22:34:35.195390Z","shell.execute_reply.started":"2024-11-29T22:33:18.892332Z","shell.execute_reply":"2024-11-29T22:34:35.194502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_accuracy, test_loss, test_auc, fpr, tpr = test(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T22:36:34.878461Z","iopub.execute_input":"2024-11-29T22:36:34.878820Z","iopub.status.idle":"2024-11-29T22:37:26.210032Z","shell.execute_reply.started":"2024-11-29T22:36:34.878788Z","shell.execute_reply":"2024-11-29T22:37:26.209164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_auc(fpr, tpr, auc_value):\n    plt.figure(figsize=(5, 4))\n    plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {auc_value:.4f}')\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)  # Diagonal line\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.grid(alpha=0.3)\n    plt.show()   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T22:37:40.060637Z","iopub.execute_input":"2024-11-29T22:37:40.061063Z","iopub.status.idle":"2024-11-29T22:37:40.067106Z","shell.execute_reply.started":"2024-11-29T22:37:40.061026Z","shell.execute_reply":"2024-11-29T22:37:40.066230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_auc(fpr, tpr, test_auc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T22:38:06.071669Z","iopub.execute_input":"2024-11-29T22:38:06.072443Z","iopub.status.idle":"2024-11-29T22:38:06.301848Z","shell.execute_reply.started":"2024-11-29T22:38:06.072412Z","shell.execute_reply":"2024-11-29T22:38:06.301048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\ndef plot_confusion_matrix(cm, class_names):\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T22:38:13.920337Z","iopub.execute_input":"2024-11-29T22:38:13.920698Z","iopub.status.idle":"2024-11-29T22:38:14.169342Z","shell.execute_reply.started":"2024-11-29T22:38:13.920638Z","shell.execute_reply":"2024-11-29T22:38:14.168694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrix(cm, class_names=['benign', 'malignant'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T22:38:18.882368Z","iopub.execute_input":"2024-11-29T22:38:18.883217Z","iopub.status.idle":"2024-11-29T22:38:19.122880Z","shell.execute_reply.started":"2024-11-29T22:38:18.883184Z","shell.execute_reply":"2024-11-29T22:38:19.121983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"resnet18","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\ntotal_epochs = 50\nlr = 0.01\nmomentum = 0.9\nno_cuda = False\nnum_classes=2\nlog_interval = 10\nl2_decay = 0.01\nmodel = Resnet18(num_classes=num_classes)\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T22:39:51.281062Z","iopub.execute_input":"2024-11-29T22:39:51.281905Z","iopub.status.idle":"2024-11-29T22:39:51.495172Z","shell.execute_reply.started":"2024-11-29T22:39:51.281870Z","shell.execute_reply":"2024-11-29T22:39:51.494281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink\n\n# Model training\nmodel.to(device)  \n\nbest_accuracy = 0\nearly_stop = EarlyStopping(patience=20, verbose=True)\n\nproject_name = 'tumor_classification'\nmodel_name = 'resnet18'\n\n# Set Kaggle working directory\nos.chdir(r'/kaggle/working')\n\nfor epoch in range(1, total_epochs + 1):\n    # Training step\n    train(epoch, model, total_epochs, train_loader, criterion, l2_decay)\n\n    # Validation step\n    with torch.no_grad():\n        test_loss, accuracy, cm, auc = validation(model, val_loader)\n\n    # Handle model state for single/multiple GPUs\n    model_state_dict = model.module.state_dict() if isinstance(model, nn.parallel.DistributedDataParallel) else model.state_dict()\n\n    # Save directory for models\n    model_save_dir = os.path.join('model', project_name, model_name)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n\n    # Early stopping check\n    early_stop(test_loss, model)\n\n    # Save the best model based on AUC\n    if auc > best_accuracy:\n        best_accuracy = auc\n        model_save_path = os.path.join(model_save_dir, f'{model_name}_epoch_{epoch}.pth')\n        torch.save(model_state_dict, model_save_path, _use_new_zipfile_serialization=False)\n        print(f\"Model saved at: {os.path.abspath(model_save_path)}\")\n\n        # Generate download link for Kaggle\n        print(\"Generating download link for the saved model...\")\n        display(FileLink(model_save_path))\n\n    # Stop training if early stopping is triggered\n    if early_stop.early_stop:\n        print(\"Early stopping\")\n        break\n\n# Save the model at the final epoch if early stopping is triggered\nfinal_model_save_path = os.path.join(model_save_dir, f'{model_name}_final_epoch.pth')\ntorch.save(model_state_dict, final_model_save_path, _use_new_zipfile_serialization=False)\nprint(f\"Final model saved at: {os.path.abspath(final_model_save_path)}\")\n\n# Generate download link for the final model\nprint(\"Generating download link for the final model...\")\ndisplay(FileLink(final_model_save_path))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import metrics\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nimport torch\n\nimport numpy as np\n\nimport torch.nn.functional as F\n\n\n\ndef test(model, test_loader):\n    name = 'test'\n    len_test_loader = len(test_loader.dataset)\n    model.eval()\n\n    test_loss = 0\n    correct = 0\n    possibilities = None\n    all_predictions = []\n    true_labels = []  # Collect true labels here\n    class_names = ['benign', 'malignant']\n\n    for data, target in test_loader:\n        if torch.cuda.is_available():\n            data, target = data.cuda(), target.cuda()\n\n        test_output = model(data)\n        test_loss += F.nll_loss(F.log_softmax(test_output, dim=1), target, reduction='sum').item()\n\n        pred = test_output.data.max(1)[1]\n        all_predictions.extend(pred.cpu().numpy())  # Collect predictions\n        true_labels.extend(target.cpu().numpy())  # Collect true labels\n\n        possibility = F.softmax(test_output, dim=1).cpu().data.numpy()\n        if possibilities is None:\n            possibilities = possibility\n        else:\n            possibilities = np.concatenate((possibilities, possibility), axis=0)\n\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    # Ensure all_predictions and true_labels are flattened\n    all_predictions = np.array(all_predictions)\n    true_labels = np.array(true_labels)\n\n    # Classification metrics -> accuracy, F1 score\n    print(metrics.classification_report(true_labels, all_predictions, target_names=class_names, digits=4))\n\n    # Confusion matrix\n    cm = metrics.confusion_matrix(true_labels, all_predictions)\n    print(\"\\nConfusion Matrix:\\n\", cm)\n\n    # ROC Curve and AUC\n    num_classes = len(class_names)\n    label_onehot = np.eye(num_classes)[true_labels.astype(int)]\n    fpr, tpr, thresholds = roc_curve(label_onehot.ravel(), possibilities.ravel())\n    auc_value = roc_auc_score(label_onehot, possibilities, average=\"macro\")\n\n    # Specificity, Sensitivity, AUC\n    test_loss /= len_test_loader\n    print('Specificity: {:.4f}, Sensitivity: {:.4f}, AUC: {:.4f}'.format(1 - fpr[0], tpr[0], auc_value))\n    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        name, test_loss, correct, len_test_loader, 100. * correct / len_test_loader))\n\n    #return 100. * correct / len_test_loader, test_loss, auc_value\n    return 100. * correct / len_test_loader, test_loss, auc_value, fpr, tpr\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T22:40:47.289540Z","iopub.execute_input":"2024-11-29T22:40:47.289913Z","iopub.status.idle":"2024-11-29T22:40:47.299710Z","shell.execute_reply.started":"2024-11-29T22:40:47.289878Z","shell.execute_reply":"2024-11-29T22:40:47.298806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_accuracy, test_loss, test_auc, fpr, tpr = test(model, test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_auc(fpr, tpr, auc_value):\n    plt.figure(figsize=(5, 4))\n    plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {auc_value:.4f}')\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)  # Diagonal line\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.grid(alpha=0.3)\n    plt.show()   \nplot_auc(fpr, tpr, test_auc)\n\nimport seaborn as sns\n\ndef plot_confusion_matrix(cm, class_names):\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    plt.show()\nplot_confusion_matrix(cm, class_names=['benign', 'malignant'])    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"resnet 50","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\n\n# For ResNet18\nresnet50 = models.resnet50(pretrained=True)\nresnet50.eval()  # Set the model to evaluation mode\nprint(\"ResNet50 Model:\")\nprint(resnet50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\nimport torchvision.models as models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Resnet50(nn.Module):\n    def __init__(self, num_classes=2):\n        super(Resnet50, self).__init__()\n        model_resnet50 = models.resnet50(pretrained=True)\n        self.conv1 = model_resnet50.conv1\n        self.bn1 = model_resnet50.bn1\n        self.relu = model_resnet50.relu\n        self.maxpool = model_resnet50.maxpool\n        self.layer1 = model_resnet50.layer1\n        self.layer2 = model_resnet50.layer2\n        self.layer3 = model_resnet50.layer3\n        self.layer4 = model_resnet50.layer4\n        self.avgpool = model_resnet50.avgpool\n        self.features = model_resnet50.fc.in_features\n        self.fc = nn.Linear(self.features, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Resnet50()\n\nprint(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"next run \n1 early stop\n2 train fun\n3 val fun","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\ntotal_epochs = 50\nlr = 0.01\nmomentum = 0.9\nno_cuda = False\nnum_classes=2\nlog_interval = 10\nl2_decay = 0.01\n#model = customVGG16(num_classes=num_classes)\nmodel = Resnet50(num_classes=num_classes)\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink\n\n# Model training\nmodel.to(device)  \n\nbest_accuracy = 0\nearly_stop = EarlyStopping(patience=20, verbose=True)\n\nproject_name = 'tumor_classification_3'\nmodel_name = 'resnet18'\n\n# Set Kaggle working directory\nos.chdir(r'/kaggle/working')\n\nfor epoch in range(1, total_epochs + 1):\n    # Training step\n    train(epoch, model, total_epochs, train_loader, criterion, l2_decay)\n\n    # Validation step\n    with torch.no_grad():\n        test_loss, accuracy, cm, auc = validation(model, val_loader)\n\n    # Handle model state for single/multiple GPUs\n    model_state_dict = model.module.state_dict() if isinstance(model, nn.parallel.DistributedDataParallel) else model.state_dict()\n\n    # Save directory for models\n    model_save_dir = os.path.join('model', project_name, model_name)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n\n    # Early stopping check\n    early_stop(test_loss, model)\n\n    # Save the best model based on AUC\n    if auc > best_accuracy:\n        best_accuracy = auc\n        model_save_path = os.path.join(model_save_dir, f'{model_name}_epoch_{epoch}.pth')\n        torch.save(model_state_dict, model_save_path, _use_new_zipfile_serialization=False)\n        print(f\"Model saved at: {os.path.abspath(model_save_path)}\")\n\n        # Generate download link for Kaggle\n        print(\"Generating download link for the saved model...\")\n        display(FileLink(model_save_path))\n\n    # Stop training if early stopping is triggered\n    if early_stop.early_stop:\n        print(\"Early stopping\")\n        break\n\n# Save the model at the final epoch if early stopping is triggered\nfinal_model_save_path = os.path.join(model_save_dir, f'{model_name}_final_epoch.pth')\ntorch.save(model_state_dict, final_model_save_path, _use_new_zipfile_serialization=False)\nprint(f\"Final model saved at: {os.path.abspath(final_model_save_path)}\")\n\n# Generate download link for the final model\nprint(\"Generating download link for the final model...\")\ndisplay(FileLink(final_model_save_path))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import metrics\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nimport torch\n\nimport numpy as np\n\nimport torch.nn.functional as F\n\n\n\ndef test(model, test_loader):\n    name = 'test'\n    len_test_loader = len(test_loader.dataset)\n    model.eval()\n\n    test_loss = 0\n    correct = 0\n    possibilities = None\n    all_predictions = []\n    true_labels = []  # Collect true labels here\n    class_names = ['benign', 'malignant']\n\n    for data, target in test_loader:\n        if torch.cuda.is_available():\n            data, target = data.cuda(), target.cuda()\n\n        test_output = model(data)\n        test_loss += F.nll_loss(F.log_softmax(test_output, dim=1), target, reduction='sum').item()\n\n        pred = test_output.data.max(1)[1]\n        all_predictions.extend(pred.cpu().numpy())  # Collect predictions\n        true_labels.extend(target.cpu().numpy())  # Collect true labels\n\n        possibility = F.softmax(test_output, dim=1).cpu().data.numpy()\n        if possibilities is None:\n            possibilities = possibility\n        else:\n            possibilities = np.concatenate((possibilities, possibility), axis=0)\n\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    # Ensure all_predictions and true_labels are flattened\n    all_predictions = np.array(all_predictions)\n    true_labels = np.array(true_labels)\n\n    # Classification metrics -> accuracy, F1 score\n    print(metrics.classification_report(true_labels, all_predictions, target_names=class_names, digits=4))\n\n    # Confusion matrix\n    cm = metrics.confusion_matrix(true_labels, all_predictions)\n    print(\"\\nConfusion Matrix:\\n\", cm)\n\n    # ROC Curve and AUC\n    num_classes = len(class_names)\n    label_onehot = np.eye(num_classes)[true_labels.astype(int)]\n    fpr, tpr, thresholds = roc_curve(label_onehot.ravel(), possibilities.ravel())\n    auc_value = roc_auc_score(label_onehot, possibilities, average=\"macro\")\n\n    # Specificity, Sensitivity, AUC\n    test_loss /= len_test_loader\n    print('Specificity: {:.4f}, Sensitivity: {:.4f}, AUC: {:.4f}'.format(1 - fpr[0], tpr[0], auc_value))\n    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        name, test_loss, correct, len_test_loader, 100. * correct / len_test_loader))\n\n    #return 100. * correct / len_test_loader, test_loss, auc_value\n    return 100. * correct / len_test_loader, test_loss, auc_value, fpr, tpr\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_accuracy, test_loss, test_auc, fpr, tpr = test(model, test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_auc(fpr, tpr, auc_value):\n    plt.figure(figsize=(5, 4))\n    plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {auc_value:.4f}')\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)  # Diagonal line\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.grid(alpha=0.3)\n    plt.show()   \nplot_auc(fpr, tpr, test_auc)\n\nimport seaborn as sns\n\ndef plot_confusion_matrix(cm, class_names):\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    plt.show()\nplot_confusion_matrix(cm, class_names=['benign', 'malignant'])    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}